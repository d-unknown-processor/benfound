\documentclass[twoside,leqno,twocolumn]{article}

\newcommand{\algoname}{\textsc{BenFound}}
\newcommand{\eDiv}{\textsc{eDiv}}
\newcommand{\eAgglo}{\textsc{eAgglo}}
\newcommand{\cpppo}{\textsc{cp3o}}
\newcommand{\pDPA}{\textsc{pDPA}}
\newcommand{\TED}{\textsc{TED}}
\newcommand{\BinSegMean}{\textsc{BinSeg-Mean}}
\newcommand{\BinSeg}{\textsc{BinSeg}}
\newcommand{\BinSegVar}{\textsc{BinSeg-Var}}
\newcommand{\BinSegMeanVar}{\textsc{BinSeg-MeanVar}}

\usepackage{ltexpprt}

\usepackage{amsfonts}

\begin{document}

<<setup, cache=FALSE, echo=F,message=F>>=
library(Hmisc)
library(knitr)
source('load.R')
source('func.R')
# global chunk options
opts_chunk$set(cache=TRUE, autodep=TRUE)

# a function to get the first digit of an non-zero integer
getFirstDigit <- function(k){ as.numeric(head(strsplit(as.character(abs(k)),'')[[1]],n=1))}

benfordProbabilitiesBase10 <- log(1+(1/(1:9)),10)
benfordTwoDigitProbabilitiesBase10 <-  getTwoDigitBenfordDensities()

@ 

<<setup-extended-version,echo=F,message=F>>=
isExtendedVersion <- FALSE
@ 
% \setcounter{chapter}{2} % If you are doing your chapter as chapter one,
% \setcounter{section}{3} % comment these two lines out.

\title{Let's see your Digits: Anomalous-State Detection using Benford's Law}
\author{Samuel Maurus\thanks{Helmholtz Zentrum Munich, Technical University of Munich, samuel.maurus@helmholtz-muenchen.de} \\
  \and
  Claudia Plant\thanks{University of Vienna, claudia.plant@univie.ac.at}}
\date{}

\maketitle


% \pagenumbering{arabic}
% \setcounter{page}{1}%Leave this line commented out.

\begin{abstract} \small\baselineskip=9pt Benford's Law explains a curious ``naturally-occurring'' phenomenon in which the leading digits of numerical data are distributed in a precise fashion. In this paper we begin by showing that system metrics generated by many modern information systems like Twitter, Wikipedia, YouTube and GitHub obey this law. We then propose a novel unsupervised approach called \algoname{} that exploits this property to detect anomalous system events. \algoname{} tracks the ``Benfordness'' of key system metrics, like the follower counts of tweeting Twitter users or the change deltas in Wikipedia page edits. It then applies a novel Benford-conformity test in real-time to identify ``non-Benford events''. We investigate a variety of such events, showing that they correspond to unnatural and often undesired system interactions like spamming, hashtag-hijacking and denial-of-service attacks. The result is a technically-uncomplicated and effective  ``red flagging'' technique. Although not without its limitations, it is highly efficient and requires neither obscure parameters, text streams nor natural-language processing.\end{abstract}

\section{Introduction}
\label{sec:introduction}
% no \IEEEPARstart

In various domains it is useful to know if the interactions with a running system deviate from expected patterns. A canonical example is financial fraud, where organizations have an interest in exposing any persons attempting to deceive their funding and accounting processes. Other examples include online collaborative ecosystems like Twitter and Wikipedia, which strive to promptly detect and suppress behavior like spamming and attacks which can degrade service quality \cite{twitterTos}.

On the surface, such manipulative behavior may appear normal and be difficult to detect. A tax cheat may fabricate figures that, although dishonest, lie within expected ranges. Analogously, a spammer may use a team of fake Twitter accounts, each of which have sensible numbers of followers and friends. The manner in which the \textit{absolute values} of such numbers are distributed is often not unusual.

The key to the effectiveness of Benford's Law (BL) in such situations lies with its observation that the \textit{leading digits} of many sets of numbers are, by nature, distributed in a surprisingly \textit{non-uniform} way. In contrast, if such numbers are \textit{fabricated}, the leading digits rarely follow that same distribution. For example, the success of BL in financial auditing has shown that the na\"{\i}ve fraudster, in an attempt to contrive believable figures that are varied and ``well spread'', often fabricates figures with a more or less uniform distribution of leading digits \cite{nigrini2012}. When doing a Benford analysis, we focus \textit{only} on the leading digits and \textit{purposefully discard the magnitude information}. In this work we propose that monitoring the resulting digit distribution is a novel, robust and elegant mechanism for tracking the integrity of complex systems.

<<twitter-count-data, echo=F>>=
twitterCountsData <- read.csv("data/intro_twitter_counts.csv", header=FALSE)
friendsCountVector <- as.numeric(twitterCountsData[,1])
friendsCountVector <- friendsCountVector[friendsCountVector != 0]
statusesCountVector <- as.numeric(twitterCountsData[,2])
statusesCountVector <- statusesCountVector[statusesCountVector != 0]
followersCountVector <- as.numeric(twitterCountsData[,3])
followersCountVector <- followersCountVector[followersCountVector != 0]
favoritesCountVector <- as.numeric(twitterCountsData[,4])
favoritesCountVector <- favoritesCountVector[favoritesCountVector != 0]
listedCountVector <- as.numeric(twitterCountsData[,5])
listedCountVector <- listedCountVector[listedCountVector != 0]
@ 

% Here we display the first histogram
<<twitter-count-distributions, fig.cap='First-digit distributions for the five count-based metrics of 15,000 randomly-chosen Twitter users (friend-, status-, follower-, favorite- and listed-count). The bars represent the measured frequencies, and the filled bullets those predicted by Benford\'s Law.', fig.width=6, fig.height=5, fig.pos='t!', echo=F>>=
friendsFirstDigits <- c()
for(f in friendsCountVector){friendsFirstDigits <- c(friendsFirstDigits, getFirstDigit(f));}
statusesFirstDigits <- c()
for(f in statusesCountVector){statusesFirstDigits <- c(statusesFirstDigits, getFirstDigit(f));}
followersFirstDigits <- c()
for(f in followersCountVector){followersFirstDigits <- c(followersFirstDigits, getFirstDigit(f));}
favoritesFirstDigits <- c()
for(f in favoritesCountVector){favoritesFirstDigits <- c(favoritesFirstDigits, getFirstDigit(f));}
listedFirstDigits <- c()
for(f in listedCountVector){listedFirstDigits <- c(listedFirstDigits, getFirstDigit(f));}

layout(matrix(c(1,1,1,2,2,2,3,3,4,4,5,5),2,6,byrow=TRUE))
hist(friendsFirstDigits, seq(0.5, 9.5, 1), xlim=c(0,10), xlab="First significant digit", main="Friends count (1st digit)",family="Times", xaxt="n")
points(1:9,length(friendsFirstDigits)*benfordProbabilitiesBase10, pch=16)
axis(1, at=1:9, labels=as.character(1:9), family="Times")
hist(statusesFirstDigits, seq(0.5, 9.5, 1), xlim=c(0,10), xlab="First significant digit", main="Status count (1st digit)",family="Times", xaxt="n")
points(1:9,length(statusesFirstDigits)*benfordProbabilitiesBase10, pch=16)
axis(1, at=1:9, labels=as.character(1:9), family="Times")
hist(followersFirstDigits, seq(0.5, 9.5, 1), xlim=c(0,10), xlab="First significant digit", main="Followers count (1st digit)",family="Times", xaxt="n")
points(1:9,length(followersFirstDigits)*benfordProbabilitiesBase10, pch=16)
axis(1, at=1:9, labels=as.character(1:9), family="Times")
hist(favoritesFirstDigits, seq(0.5, 9.5, 1), xlim=c(0,10), xlab="First significant digit", main="Favorites count (1st digit)",family="Times", xaxt="n")
points(1:9,length(favoritesFirstDigits)*benfordProbabilitiesBase10, pch=16)
axis(1, at=1:9, labels=as.character(1:9), family="Times")
hist(listedFirstDigits, seq(0.5, 9.5, 1), xlim=c(0,10), xlab="First significant digit", main="Listed count (1st digit)",family="Times", xaxt="n")
points(1:9,length(listedFirstDigits)*benfordProbabilitiesBase10, pch=16)
axis(1, at=1:9, labels=as.character(1:9), family="Times")
@ 

To grasp the basic concept of BL, we share measurements collected from 15,000 randomly-chosen Twitter users. If we consider the number of followers that each user has, we may see that one user has \textbf{2}93,845 followers while another has only \textbf{3}20. As an exercise in curiosity, how would we expect the distribution to appear when tallying just the first significant digit (marked in \textbf{bold}) of each value? That is, how likely is it that one of our 15,000 users has a follower-count that begins with the digit $9$ versus the digit $1$? To those unfamiliar with BL, the usual answer is ``equally likely''.

As Figure \ref{fig:twitter-count-distributions} shows, however, the distribution is far from uniform. Indeed, it defies intuition: it tells us that finding a follower-count starting with a $1$ is \textit{over six times more likely} than finding one starting with a $9$. This observation is not unique to this sample. In the same figure we see the distributions for the four other Twitter count-based metrics for each user (friend, listed, status, and favorited counts). All have the same basic form, and we can visually see a strong agreement with the BL predictions in each case. Considering that data from services such as Twitter are considered noisy \cite{antoine2015portraying, hayashi2015real}, this strong BL-conformity is a useful property.

% A note about the collection of the Twitter data. The raw data contains 14698 users with five measurements for each. In order, they are 1) friends_count, 2) statuses_count, 3) followers_count, 4) favorites_count and 5) listed_count.
% Note that we omit zero measurements for obvious reasons (they have no first significant digit)
% Data collection: we used the user-search feature of the Twitter API in March 2016. The search endpoint only allows a maximum of 1000 results returned and a search query is required. For this reason we used 15 of the most common trigraphs in the English language as the search terms: ["the", "and", "tha", "ent", "ion", "tio", "for", "nde", "has", "nce", "edt", "tis", "oft", "sth", "men"]. These were chosen based on the data presented on http://www.rollingr.net/wordpress/2007/02/02/common-letter-sequence/. For each search term (trigraph), we harvested the five metrics from the 1000 users returned from the endpoint. 
% Although we concede that this is not the most ideal random-sampling strategy, we argue that it is reasonable given the constraints of the Twitter API. We are confident that any other sensible sampling technique would yield very similar results (with respect to obeying bBenford's law).

In this paper we exploit the BL-conformity of such systems to form an intriguing foundation for describing what it means for systems to be in their ``expected'' or  ``natural'' state. We then present an approach for the detection of ``unexpected'' or  ``unnatural'' deviations from that state. Just as Benford's Law can expose fraudulent behavior in financial reports not seen by other tools, we show that the events detected by our framework are not found by state-of-the-art anomaly- and event-detection techniques. \newline

\noindent \textbf{Contributions:}

\noindent\textbf{1) We show the Benfordness of key metrics} tracked by Twitter, Wikipedia, YouTube and GitHub (Sections \ref{sec:introduction}, \ref{sec:case-study:-github} and \ref{sec:bot-detection-using-benfords-law}).

\noindent\textbf{2) We propose a test for Benford's Law conformity} exploiting the law's logarithmic basis and the formal Kolgomorov-Smirnov test (Section \ref{sec:test-conf-benf}).

\noindent\textbf{3) We present \algoname{} to detect ``unnatural'' events from streams of \textit{numerical data}}. \algoname{} does not require a text stream, nor any parameters other than the width $w$ of the sliding window (Section \ref{sec:extr-benf-sign}). It can be deployed in real-time and has linear run-time complexity in $w$.

\noindent\textbf{4) We present experiments on synthetic data}, showing that \algoname{} finds events not visible to state-of-the-art change-point and event-detection techniques that feed on numerical data (Section \ref{sec:synth-exper}).

\noindent\textbf{5) We present a number of real-world case-studies} showing how \algoname{} detects ``unnatural'' events not found by other techniques (Sections \ref{sec:bot-detection-using-benfords-law}, \ref{sec:case-study:-twitter}, \ref{sec:case-study:-outag}).


\section{Preliminaries}
\label{sec:preliminaries}

Benford's Law asserts that the probabilities for finding each of the nine possible digits $1,2,\ldots,9$ as the first significant digit are \textit{not equal}. It states that the digit 1, for example, occurs more than 30\% of the time and the digit 9 less than 5\% of the time. More precisely, given a vector $\vec{x} \in \mathbb{R}^n$ of numerical values, BL states that the probability of a randomly-selected element from $\vec{x}$ having the \textit{first} significant digit $d \in \{1,\ldots,9\}$ is

\begin{equation}
  \label{eq:bl-first-digit}
  P(D_1 = d) = \log_{10} (d+1) - \log_{10} (d).
\end{equation}

The bullets ($\bullet$) in Figure \ref{fig:twitter-count-distributions} were computed using these probabilities. The general form of BL further specifies the joint probability distribution of \textit{all} the significant digits \cite{berger2015Intro}\footnote{Note that the distributions for the second and higher digits quickly approach the uniform.}. 

Although it is clear that not \textit{all} sets of numbers obey the law (a sample of human heights in cm, for example, does not), Benford's original manuscript \cite{benford1938law} includes evidence supporting its wide application. Data from diverse sources as baseball statistics, death rates, lists of physical constants and randomly-selected numbers from newspapers were shown to conform. We recommend that the justifiably-skeptical reader perform a simple experiment in this light, such as randomly sampling numerical data from independent web pages.

Many familiar sequences follow BL. These include Fibonacci, the powers of (almost\footnote{For base ten it is clear that powers of any integer that is itself a power of ten (e.g. 1000) will not obey the law.}) any integer, and the factorials ($1!, \ldots, n!$). Such sequences offer an informal starting point for \textit{explaining} BL. It turns out that BL is often (but not always) observed to hold for sets of numbers spanning several orders of magnitude and associated with exponential or multiplicative growth processes. In this light, consider the sequence of numbers generated by the powers of 1.2 (that is, $1.2^1,1.2^2,1.2^3,\ldots$) plotted on a logarithmic axis:
\noindent
<<log-powers-of-two, fig.width=10, fig.height=3, fig.pos='h!', echo=F>>=
plot(log10(1.2^(0:37)), replicate(38, 1), xlim=c(0,3), yaxt="n", ylab="", xlab="",xaxt="n",bty="n",pch=21,bg=1,cex=1.2); axis(1,at=c(0:3), labels=as.character(c("1","10","100","1000"))); axis(1,at=log10(c(2:9,seq(20,90,10),seq(200,900,100))), labels=as.character(c("2","3","","","","","","","20","30","","","","","","","200","300","","","","","","")),cex.axis=1); rect(log10(c(1:9,seq(10,90,10),seq(100,900,100))),0,log10(c(2:10,seq(20,100,10),seq(200,1000,100))),2,col=replicate(3,c("#FFB30044","#803E7544","#FF680044","#A6BDD744","#C1002044","#CEA26244","#81706644","#F4C80044","#007D3444")),border=NA);
@ 

It is clear that the values are equally spread out when viewed on a logarithmic scale. It is then obvious that one of these values is more likely to fall into an interval where the first significant digit is one, that is, one of the intervals $[1,2)$, $[10,20)$, $[100,200)$ and so on. We can now understand the BL formula in Equation \ref{eq:bl-first-digit} graphically: the probability for any given digit is simply the width of its interval on the logarithmic scale.

With this in mind, let us consider the physiological and psychological reaction to external stimuli that we find in the real world. The growth of the sensation of brightness (Fechner's Law), the sense of loudness, the sense of weight, the response of the body to medicine or radiation, and the killing curves under toxins and radiation are all often logarithmic. In economics we often hear about percentage rates of growth. In social media, content can go ``viral'' with an exponentially growing number of views or downloads over time. As Benford himself elegantly noted: ``the analogy is complete, and one is tempted to think that the $1, 2, 3, \ldots$ scale is not the natural scale; but that, invoking the base $e$ of the natural logarithms, \textit{Nature} counts $e^0, e^x, e^{2x}, e^{3x}, \ldots$ and builds and functions accordingly'' \cite{benford1938law}.

Given that BL arises naturally, it should come as no surprise that it is base- and unit-invariant. This means that numbers obeying BL keep obeying BL after we express them in a different base (e.g. octal) or in different units (e.g. feet instead of meters). The mathematical properties of the law are intriguing, and we refer the curious reader to \cite{berger2015Intro} for a formal treatment. Although many facets of BL now rest on solid ground, there remains \textit{no unified approach} that simultaneously explains its appearance in dynamical systems, number theory, statistics and real-world data \cite{berger2011benford}. 

In this work we focus on \textit{applications} of the law, so to simplify matters we focus on decimal numbers with no scaling applied. Arguably the most classical application for BL is in fraud-detection. For example, an employee who serially alters the leading digits from 1 to 7 on reimbursement invoices (such that a hotel stay becomes \$738.45 instead of \$138.45, for example) introduces an artificial and detectable bias to the leading-digits distribution \cite{nigrini2012}.

Our work is based on the idea that different kinds of unnatural events take place in different domains. These events may or may not be malicious in nature. In Twitter, for example, we will see that the lead-up to Father's Day is accompanied by artificial use of the \textbf{\#FathersDay} hashtag for pushing product sales (spamming and advertising). In such cases we can raise a ``red-flag'' and investigate the items that violate the digit distribution.

\section{Further Evidence of Benford's Law Online}
\label{sec:case-study:-github}

<<gitHub-setup,echo=F,warning=F>>=
# The columns in githubData represent full_name, stargazers_count, watchers_count, forks_count, open_issues_count respectively. Watchers count seems to be identical to stargazers count in the data.
githubData <- read.csv("data/github-2016-07-18.csv",header=FALSE)
starsCount <- as.numeric(na.omit(githubData[,2]))
starsCountFirstDigits <- getTwoSignificantDigits(starsCount[starsCount>=10])
forksCount <- as.numeric(na.omit(githubData[,4]))
forksCountFirstDigits <- getTwoSignificantDigits(forksCount[forksCount>=10])
issuesCount <- as.numeric(na.omit(githubData[,5]))
issuesCountFirstDigits <- getTwoSignificantDigits(issuesCount[issuesCount>=10])


## Now the YouTube data. Here the columns are id, view count, like count, dislike count, favorite count, comment count. The favorites seem to always be zero in the data (bug in YouTube API?), so we skip them.
youtubeData <- read.csv("/home/sam/Documents/phd/projects/bend-it-like-benford/tex/data/youtube-2016-07-18-snapshot-2016-07-19.csv",header=FALSE)
viewsCount <- as.numeric(na.omit(youtubeData[,2]))
viewsCountFirstDigits <- getTwoSignificantDigits(viewsCount[viewsCount>=10])
likesCount <- as.numeric(na.omit(youtubeData[,3]))
likesCountFirstDigits <- getTwoSignificantDigits(likesCount[likesCount>=10])
dislikesCount <- as.numeric(na.omit(youtubeData[,4]))
dislikesCountFirstDigits <- getTwoSignificantDigits(dislikesCount[dislikesCount>=10])
commentsCount <- as.numeric(na.omit(youtubeData[,6]))
commentsCountFirstDigits <- getTwoSignificantDigits(commentsCount[commentsCount>=10])

options(digits=1)
@ 


Figure \ref{fig:github-data-digit-distributions} shows that, like the Twitter data in our Introduction, the count metrics from YouTube videos and GitHub repositories are close to Benford. For this experiment we collected data from the YouTube\footnote{developers.google.com/youtube/v3/} and GitHub\footnote{developer.github.com/v3/} APIs. We provide the data for reuse and result-reproduction\footnote{\label{supMatFootnote}This manuscript was created using \textit{Knitr}, the engine for dynamic report generation with R. The downloadable source for this document, including all data and embedded R code necessary to generate the results, tables and figures, is available at [todo].}. 

<<github-data-digit-distributions, fig.env='figure',fig.width=10, fig.height=12, fig.pos='h', fig.cap='From top left: Leading two-digit histograms for YouTube Views, Likes, Dislikes, Comments, and GitHub Stars and Forks. Each histogram bar on the horizontal axis represents one of the 90 possible leading two digit combinations (10-99).', echo=F, warning=F>>=
layout(matrix(c(1,2,3,4,5,6),3,2,byrow=TRUE))

# youtube views
hist(viewsCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="YouTube Views",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
points(10:99,length(viewsCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)

# youtube likes
hist(likesCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="YouTube Likes",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
points(10:99,length(likesCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)

# youtube dislikes
hist(dislikesCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="YouTube Dislikes",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
points(10:99,length(dislikesCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)

# youtube comments
hist(commentsCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="YouTube Comments",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
points(10:99,length(commentsCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)

# github stars
hist(starsCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="GitHub Stars",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
points(10:99,length(starsCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)

# github forks
hist(forksCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="GitHub Forks",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
points(10:99,length(forksCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)

## # github issues
## hist(issuesCountFirstDigits, seq(9.5, 100.5, 1), xlim=c(9.5,100.5), xlab="", main="GitHub Issues",family="Times",ylab="", xaxt="n",cex.main=3,cex.lab=2,cex.axis=2)
## points(10:99,length(issuesCountFirstDigits)*benfordTwoDigitProbabilitiesBase10, pch=16,cex=2)
@

% Interestingly, we see that the level of conformity correlates with the range of the metric in question. That is, the YouTube view counts range up to the large value of $\Sexpr{signif(max(viewsCount),2)}$ and are seen to be a close to perfect fit to Benford's prediction. YouTube likes, dislikes and comment counts, however, span a narrower range. Their maximum values are $\Sexpr{signif(max(likesCount),2)}$, $\Sexpr{signif(max(dislikesCount),3)}$ and $\Sexpr{signif(max(commentsCount),3)}$ respectively. GitHub stars and forks have even narrower ranges (up to $\Sexpr{signif(max(starsCount),4)}$, $\Sexpr{signif(max(forksCount),4)}$ and $\Sexpr{max(issuesCount)}$ respectively). This result empirically helps to support the conjecture that BL is most pronounced for data spanning many orders of magnitude.

\section{Bot-Detection using Benford's Law}
\label{sec:bot-detection-using-benfords-law}

In this section we consider the application of Benford's Law in the detection of anomalous behavior in online services in the form of \textbf{bots} (where automated computer programs, rather than humans, interact with a system). As a case study we present and share data collected from Wikipedia's real-time Recent Changes (RC) stream\footnote{mediawiki.org/wiki/API:Recent\_changes\_stream}. 

Between July 6 and July 11 2016 we collected $n>2$ million page-edit events classified by Wikipedia as \textit{non-bot} edits. We recorded the vector $\vec{\Delta}=\left(\delta_1, \delta_2, \ldots, \delta_n\right)$, where each $\delta_i$ represents the magnitude in bytes of the corresponding change. Changes ranged from a few bytes to over two megabytes. 

% Figure \ref{fig:wikipedia-data-log-histogram} shows the histogram of the \textit{logarithms} of the values in $\vec{\Delta}$.
% 
% wikipedia-data-log-histogram, fig.width=10, fig.height=5, fig.pos='h!', fig.cap='Histogram showing the distribution of the logarithms of the Wikipedia page-edit deltas',echo=F, warning=F
% wikiData <- read.csv("data/wiki-all-length-2016-07-06.csv",header=FALSE)
% # The columns in wikiData represent the server name, bot?, patrolled?, type (edit/new), minor?, old length, new length, delta, comment
% wikiNonBotEdits <- wikiData[wikiData$V2 == 0 & wikiData$V4 == "edit",]
% # Get the vector of deltas
% nonBotEditSizes <- as.numeric(wikiNonBotEdits$V8)
% # Consider the negative deltas only for this experiment (i.e. the destructive changes). Nigrini suggests always analyzing the positive and negative numbers separately. Here we'll analyze negative numbers. We take all those lte -10, so that we can do a two-digit analysis
% nonBotEditSizes <- abs(nonBotEditSizes[nonBotEditSizes<=-10])
% # Calculate the base-10 logarithm of the delta magnitudes
% logVector <- log(nonBotEditSizes,base=10)
% # Plot the histogram of the log vector
% hist(logVector, breaks=seq(0.9,7,0.17),xlab="",main="Logarithms of Page-Edit Deltas",cex.main=2,cex.lab=1.3,cex.axis=1.3)
% @ 

% We see that the distribution of logarithms has the long tail expected from this kind of data (large change deltas are less common). 

<<wikipedia-data-digits-histogram, fig.width=10, fig.height=5, fig.pos='h!', fig.cap='Leading two-digit histogram for Wikipedia page-edit deltas. Red bars are abberations from BL.', echo=F, warning=F>>=

# Here we use a file with raw Wikipedia stream data. Unfortunately it's over 100MB, so GitHub won't accept it. If you want the original file, please contact the author. We provide the vector of change deltas only, which was generated from the original file with the following code                                        
#wikiData <- read.csv("data/wiki-all-length-2016-07-06.csv",header=FALSE)
## The columns in wikiData represent the server name, bot?, patrolled?, type (edit/new), minor?, old# length, new length, delta, comment
#wikiNonBotEdits <- wikiData[wikiData$V2 == 0 & wikiData$V4 == "edit",]
## Get the vector of deltas
#nonBotEditSizes <- as.numeric(wikiNonBotEdits$V8)
## Consider the negative deltas only for this experiment (i.e. the destructive changes). Nigrini suggests always analyzing the positive and negative numbers separately. Here we'll analyze negative numbers. We take all those lte -10, so that we can do a two-digit analysis
#nonBotEditSizes <- abs(nonBotEditSizes[nonBotEditSizes<=-10])

# Here the code to just get the result straight from the smaller file we've included
nonBotEditSizes <- as.numeric(read.csv("data/wiki-all-length-2016-07-06-nonBotEditSizes.csv",header=F))
firstTwoDigits <- getTwoSignificantDigits(nonBotEditSizes); benfDensities <- getTwoDigitBenfordDensities(); wikiHistogramColoring <- replicate(90,"white"); wikiHistogramColoring[c(2,13,29,31)] <- "red"; hist(firstTwoDigits,seq(9.5,100.5,1),main="Wikipedia Change Deltas",xlab="",col=wikiHistogramColoring,cex.main=1.7,cex.axis=1.7,cex.lab=1.3); points(10:99,benfDensities*length(nonBotEditSizes),pch=20);
@ 

Consider in Figure \ref{fig:wikipedia-data-digits-histogram} the distribution of the \textit{leading two digits} of this data. From the basic form of the leading-digits distribution it is evident that the data in question fits the general shape predicted by Benford's Law (again shown by bullets $\bullet$). However, we see some clear aberrations. For example, noticeable spikes exist for the digit pairs $11, 22, 38$ and $40$. This data is therefore a candidate for ``red flagging'' and investigation.

Filtering the data to include only the editor comments for the changes with a delta beginning with the digits $40$, for example, we quickly notice a significant amount of automation happening, despite the edits being flagged by Wikipedia as \textit{non-bot}. Table \ref{tab:wikiBotCommentExamples} contains a sample of the offending comments. We see edits being made by ``Addbots'' as well as tools like HotCat and Gadget-Merge (automated tools for mass-editing Wikipedia content). The byte counts often \textit{begin} with the digits $40$ (despite their full values spanning several orders of magnitude), which manifests as the spike in Figure \ref{fig:wikipedia-data-digits-histogram}. At the very least, Benford's Law has thus uncovered what appears to be a data-integrity issue in the Wikipedia: many automated transactions are not being suitably labeled.

\begin{table}[h!]
  \centering
  \begin{tabular}{|l|l|}
    \hline    
    \textbf{Change comment} & \textbf{Bytes} \\
    \hline
    \footnotesize{\texttt{[[SpiderMum]] [[User:Addbot|Addbot]]}} & \textbf{40} \\
    \footnotesize{\texttt{Removed using [[Gadget-HotCat|HotCat]]}} & \textbf{40} \\
    \footnotesize{\texttt{Wbremoveclaims-remove [[P1012,Q902513]]}} & \textbf{40}7 \\
    \footnotesize{\texttt{[[MediaWiki:Gadget-Merge]] 0||Q75421}} & \textbf{40}62 \\    
    \footnotesize{\texttt{[[OneClickArchiver]]}} & \textbf{40}971 \\    
    \hline
  \end{tabular}
  \caption{Examples of automated Wikipedia edits}
  \label{tab:wikiBotCommentExamples}
\end{table}


\section{Testing Conformity to Benford's Law}
\label{sec:test-conf-benf}

<<conformity-tests-calculations,echo=F,warning=F,error=F>>=

commonRatioSequence1 <- 1.2
commonRatioSequence2 <- 1.772
conformityTestN <- 100
conformityTestSequence1 <- commonRatioSequence1^(1:conformityTestN)
conformityTestSequence2 <- commonRatioSequence2^(1:conformityTestN)
conformityTestSequence1FirstDigits <- getFirstDigits(conformityTestSequence1)
conformityTestSequence2FirstDigits <- getFirstDigits(conformityTestSequence2)
conformityTestSequence1FirstDigitsFrequencies <- replicate(9,0); for(i in conformityTestSequence1FirstDigits){  conformityTestSequence1FirstDigitsFrequencies[i] <- conformityTestSequence1FirstDigitsFrequencies[i]+1 }
conformityTestSequence2FirstDigitsFrequencies <- replicate(9,0); for(i in conformityTestSequence2FirstDigits){  conformityTestSequence2FirstDigitsFrequencies[i] <- conformityTestSequence2FirstDigitsFrequencies[i]+1 }
conformityTestSequence1FirstDigitsProportions <- conformityTestSequence1FirstDigitsFrequencies/conformityTestN
conformityTestSequence2FirstDigitsProportions <- conformityTestSequence2FirstDigitsFrequencies/conformityTestN
expectedProportions <- as.numeric(pbenf())
expectedFrequencies <- expectedProportions*conformityTestN

## Z-statistic formula from Nigrini (2012)
conformityTestZStats1 <- abs((abs(conformityTestSequence1FirstDigitsProportions-expectedProportions) - (1/200))/(sqrt(expectedProportions*(1-expectedProportions))/conformityTestN))
conformityTestZStats2 <- abs((abs(conformityTestSequence2FirstDigitsProportions-expectedProportions) - (1/200))/(sqrt(expectedProportions*(1-expectedProportions))/conformityTestN))

## We base our conclusions on the 5% significance level: Z-Statistic 1.96
conformityTestZStatisticConclusion1 <- if(sum(conformityTestZStats1>1.96) == 0) "Benf." else "Non-Benf."
conformityTestZStatisticConclusion2 <- if(sum(conformityTestZStats2>1.96) == 0) "Benf." else "Non-Benf."

## Chi-square statistic from Nigrini (2012)
conformityTestChiSquareValue1 <- sum((conformityTestSequence1FirstDigitsFrequencies - expectedFrequencies)^2/expectedFrequencies)
conformityTestChiSquareValue2 <- sum((conformityTestSequence2FirstDigitsFrequencies - expectedFrequencies)^2/expectedFrequencies)
## DF is 8 (nine first digit possibilities minus 1). Significance threshold is 0.05
chiSquaredTestCriticalValue <- qchisq(0.95,8)
conformityTestChiSquareConclusion1 <- if(conformityTestChiSquareValue1 > chiSquaredTestCriticalValue) "Non-Benf." else "Benf."
conformityTestChiSquareConclusion2 <- if(conformityTestChiSquareValue2 > chiSquaredTestCriticalValue) "Non-Benf." else "Benf."

## Euclidean distance test (from BenfordTests package...see method help for references)
conformityTestEdistConclusion1 <- if(edist.benftest(conformityTestSequence1)$p.value < 0.05) "Non-Benf." else "Benf."
conformityTestEdistConclusion2 <- if(edist.benftest(conformityTestSequence2)$p.value < 0.05) "Non-Benf." else "Benf."

## Hotelling T-square (from BenfordTests package...see method help for references)
conformityTestHotelConclusion1 <- if(jointdigit.benftest(conformityTestSequence1)$p.value < 0.05) "Non-Benf." else "Benf."
conformityTestHotelConclusion2 <- if(jointdigit.benftest(conformityTestSequence2)$p.value < 0.05) "Non-Benf." else "Benf."

## JP-square test (from BenfordTests package...see method help for references)
conformityTestJpSqConclusion1 <- if(jpsq.benftest(conformityTestSequence1)$p.value < 0.05) "Non-Benf." else "Benf."
conformityTestJpSqConclusion2 <- if(jpsq.benftest(conformityTestSequence2)$p.value < 0.05) "Non-Benf." else "Benf."

## mean-digit test (from BenfordTests package...see method help for references)
conformityTestMeanDigitConclusion1 <- if(meandigit.benftest(conformityTestSequence1)$p.value < 0.05) "Non-Benf." else "Benf."
conformityTestMeanDigitConclusion2 <- if(meandigit.benftest(conformityTestSequence2)$p.value < 0.05) "Non-Benf." else "Benf."

## MAD (Nigrini 2012). Here we use the first digits critical value of 0.015 published in Nigrini 2012
conformityTestMadValue1 <- sum(abs(conformityTestSequence1FirstDigitsProportions - expectedProportions))/9
conformityTestMadValue2 <- sum(abs(conformityTestSequence2FirstDigitsProportions - expectedProportions))/9
conformityTestMadConclusion1 <- if(conformityTestMadValue1 > 0.015) "Non-Benf." else "Benf."
conformityTestMadConclusion2 <- if(conformityTestMadValue2 > 0.015) "Non-Benf." else "Benf."

## Mantissa Arc test
conformityTestMaXSequence1 <- cos(2*pi*(log(conformityTestSequence1,10)-floor(log(conformityTestSequence1,10))))
conformityTestMaXSequence2 <- cos(2*pi*(log(conformityTestSequence2,10)-floor(log(conformityTestSequence2,10))))
conformityTestMaYSequence1 <- sin(2*pi*(log(conformityTestSequence1,10)-floor(log(conformityTestSequence1,10))))
conformityTestMaYSequence2 <- sin(2*pi*(log(conformityTestSequence2,10)-floor(log(conformityTestSequence2,10))))
conformityTestMaXCogSequence1 <- sum(conformityTestMaXSequence1)/conformityTestN
conformityTestMaXCogSequence2 <- sum(conformityTestMaXSequence2)/conformityTestN
conformityTestMaYCogSequence1 <- sum(conformityTestMaYSequence1)/conformityTestN
conformityTestMaYCogSequence2 <- sum(conformityTestMaYSequence2)/conformityTestN
conformityTestMaLsqSequence1 <- conformityTestMaXCogSequence1^2 + conformityTestMaYCogSequence1^2
conformityTestMaLsqSequence2 <- conformityTestMaXCogSequence2^2 + conformityTestMaYCogSequence2^2
conformityTestMaPValueSequence1 <- 1-exp(-(conformityTestMaLsqSequence1^2)*conformityTestN)
conformityTestMaPValueSequence2 <- 1-exp(-(conformityTestMaLsqSequence2^2)*conformityTestN)
conformityTestMaConclusion1 <- if(conformityTestMaPValueSequence1 > 0.05) "Non-Benf." else "Benf."
conformityTestMaConclusion2 <- if(conformityTestMaPValueSequence2 > 0.05) "Non-Benf." else "Benf."

## Our KS approach based on the mantissae
conformityTestKsLogPValue1 <- (ks.test(log(conformityTestSequence1,10)-floor(log(conformityTestSequence1,10)),"punif"))$p.value
conformityTestKsLogPValue2 <- (ks.test(log(conformityTestSequence2,10)-floor(log(conformityTestSequence2,10)),"punif"))$p.value
conformityTestKsLogConclusion1 <- if(conformityTestKsLogPValue1 < 0.05) "Non-Benf." else "Benf."
conformityTestKsLogConclusion2 <- if(conformityTestKsLogPValue2 < 0.05) "Non-Benf." else "Benf."

@ 

We require a mechanism to objectively assess a given numerical sample's conformity to BL. The ``state-of-the-art'' for such tests was recently surveyed by Nigrini \cite{nigrini2012}. Broadly speaking, one can partition BL tests into three categories: \textit{single-digit} tests, \textit{all-digits-at-once} tests and tests exploiting the \textit{logarithmic basis} of BL. To evaluate these tests, we consider two geometric progressions (i.e. $a^1,a^2,\ldots,a^n$) with common ratios $a=\Sexpr{commonRatioSequence1}$ and $\Sexpr{commonRatioSequence2}$. From Section \ref{sec:preliminaries} we know that each sequence obeys BL exactly in the limit $n \rightarrow \infty$.

Table \ref{tab:bl-conformity-tests} displays the conclusions (``Benford'' or ``Non-Benford'') that each test detailed in \cite{nigrini2012} makes for our two sequences based on a significance level of 0.05 and sequence length $n=100$.

\begin{table}[h]
  \tiny
  \centering
  \begin{tabular}[h]{|l|l|l|l|p{2.5cm}|}
    \hline
    Test & Type & Seq. 1 is & Seq. 2 is & Remarks \\
    \hline
    Z-Stat. & Single & \textbf{\Sexpr{conformityTestZStatisticConclusion1}} & \Sexpr{conformityTestZStatisticConclusion2} & \Sexpr{sum(conformityTestZStats2>1.96)} digits deviate significantly for Seq. 2  \\    
    Chi-Sq. & All & \textbf{\Sexpr{conformityTestChiSquareConclusion1}} & \Sexpr{conformityTestChiSquareConclusion2} & Seq. 2 $\chi^2=\Sexpr{round(conformityTestChiSquareValue2,1)}$ ($9-1=8$ d.o.f.)\\
    Euc. Dist. & All & \textbf{\Sexpr{conformityTestEdistConclusion1}} & \Sexpr{conformityTestEdistConclusion2} & \\
    Hot. $T^2$ & All & \textbf{\Sexpr{conformityTestHotelConclusion1}} & \Sexpr{conformityTestHotelConclusion2} & \\
    JP Sq. & All & \textbf{\Sexpr{conformityTestJpSqConclusion1}} & \Sexpr{conformityTestJpSqConclusion2} & \\
    MAD & All & \textbf{\Sexpr{conformityTestMadConclusion1}} & \Sexpr{conformityTestMadConclusion2} & Critical values from \cite{nigrini2012} \\
    Mant. Arc & Log & \textbf{\Sexpr{conformityTestMaConclusion1}} & \textbf{\Sexpr{conformityTestMaConclusion2}} &  \\
    K-S Mant. & Log & \textbf{\Sexpr{conformityTestKsLogConclusion1}} & \textbf{\Sexpr{conformityTestKsLogConclusion2}} & \textit{(Our proposed technique)}  \\
    \hline
  \end{tabular}
  \caption{Comparison of BL-conformity tests on two geometric progressions}
  \label{tab:bl-conformity-tests}
\end{table}

The tests based on the \textit{discrete digit distribution} (``Single'' digit, or ``All'' at once) fail to label the second sequence as Benford. These techniques are the Z-Statistic, Chi-Square, Euclidean Distance, Hotelling T-square, Joenssen's JP-square and Mean Absolute Deviation (MAD) tests. The conflicting conclusions are troublesome because both sequences are non-random and follow ideal exponential growth. The reason for the conflict is that we selected the second common ratio to maximize the \textit{information-loss during the discretization}. For example, we have a large information-loss when discretizing the sequence value 9.8595 to the ``first digit 9'' histogram bin. For moderate sample sizes, these discretization errors are enough to change the leading-digit distribution to the extent that these digits-based tests believe that the sequence is non-Benford.

To overcome this weakness, we argue that a BL conformity test should avoid such a discretization. In this light, we can design tests around the \textit{logarithmic basis} of BL, which states that the mantissae\footnote{The mantissa of a number is the fractional part of its logarithm.} of a Benford set are uniformly distributed over $[0,1)$.

Nigrini \cite{nigrini2012} also discusses BL tests that exploit this logarithmic basis. The first involves testing for two conditions: if the mantissae have a uniform distribution, then their mean and variance must be $\frac{1}{2}$ and $\frac{1}{12}$ respectively. Nigrini correctly comments that these conditions are insufficient (numerous non-uniform distributions satisfy them) and dismisses this approach. 

% A second mechanism is based on a regression approach  the insight that the ordered (ranked) mantissae should form a straight line from $\frac{-1}{n}$ to $\frac{n-1}{n}$ with a slope of $\frac{1}{n}$. Nigrini hence suggests performing a linear regression on the mantissae and testing the intercept and slope. However, a regression-based test ``should only be used after more research'' \cite{nigrini2012}. 

Another logarithmic-basis test discussed by Nigrini is the recent Mantissa-Arc (Mant. Arc) test \cite{alexander2009remarks}. It involves projecting the mantissae around the circumference of the unit circle and testing if the center-of-gravity differs significantly from zero. We see in Table \ref{tab:bl-conformity-tests} that the Mantissa-Arc test is able to correctly diagnose both sequences as Benford. At first glance, this test appears promising, however we soon realize that the center-of-gravity condition is insufficient. Consider, for example, the arbitrary-length sample $10^{1/4},10^{3/4},10^{1/4},10^{3/4},\ldots,10^{1/4},10^{3/4}$ having only two unique values. These two values are mapped to opposite sides of the unit circle, giving a zero center-of-gravity and incorrectly signaling \textit{perfect} Benford conformity. We therefore suggest to avoid this test.

Why not apply a test which directly compares the empirical cumulative distribution of the mantissae with the cumulative distribution function of the uniform? To the best of our knowledge, this approach has not been suggested. Specifically, we propose applying the formal Kolmogorov-Smirnov (K-S) one-sample test \cite{kolmogorov1933} on the mantissae to test whether or not they were sampled from the uniform. Here the null hypothesis is that the mantissae were drawn from the uniform. Adopting the common significance threshold $\alpha=0.05$, we interpret a $p<\alpha$ as an indicator for non-Benfordness. Consider for example our Twitter status-count mantissae:

%% Various classical and specialized tests can be used to this end. We refer to Nigrini's recent work \cite{nigrini2012} for a detailed discussion on such tests. One recommendation from this reference is the \textit{Mean Absolute Deviation} (MAD) test. This test yields a MAD statistic, which, as hinted in the name, is the average of the magnitudes of the differences between the observed digit proportions and those predicted by BL. In Figure \ref{fig:twitter-count-distributions}, for example, the statistic is simply calculated by summing up the distances between the each bullet/bar pair, and dividing by nine (the number of possible first digits). Smaller values of the MAD correspond to higher Benfordness. Although simple to compute, the MAD is not able to provide us with objective critical values for statistical significance (a key requirement for an event-detection framework).

\noindent
<<conformity-test-twitter-example, fig.width=10, fig.height=5, fig.pos='h!', echo=F, warning=F>>=
# Calculate the mantissae for the statuses data
statusMantissae <- log(statusesCountVector, 10)-floor(log(statusesCountVector,10))
# Perform the Kolmogorov-Smirnov test on the mantissae
ksr <- ks.test(statusMantissae,"punif")
# Display the mantissae as a histogram, noting the K-S statistic Dn and corresponding p-value
hist(statusMantissae, breaks=seq(0,1,0.025),main=sprintf("Histogram of Twitter Statuses Mantissae (K-S p-value of %.2f)",ksr$p.value), xlab="",cex.main=1.5,cex.lab=1.3,cex.axis=1.3);
@ 

The $p$-value corresponding to the application of the K-S test on this sample is 0.31. We hence accept our null hypothesis that the sample was taken from a uniform distribution, and hence that the sample conforms to BL. This same approach correctly diagnoses both our test sequences as Benford (final row in Table \ref{tab:bl-conformity-tests}). We note that the application of the K-S test on a sample involves no obscure parameters -- we only need a significance threshold $\alpha$ in order to reach a conclusion for a sample.

\section{From a Time Series to a Benfordness Signal}
\label{sec:extr-benf-sign}

Equipped with a tool to measure the Benfordness of a univariate sample, we now turn to the real-time detection of ``non-Benford'' events. That is, we wish to be alerted when a significant deviation from the ``natural'' state of a running system occurs. 

To achieve this, we track key metrics over time (e.g. follower counts of tweeting users) and apply the K-S Test on a sliding time window. The time window contains a fixed number $w$ of the newest numeric values. For the sample of size $w$ corresponding to each window, we compute the mantissae and apply the K-S test as discussed in Section \ref{sec:test-conf-benf}. The resulting $p$-value is the quantity that we then follow over time. We interpret a drop of the $p$-value below a significance threshold $\alpha$ as an indicator for the beginning of ``non-Benford'' behavior in the system (something unnatural that should be investigated).

\textbf{Window Size $w$ and Threshold $\alpha$:} The value for $w$ is user-specified. The K-S test we use for BL-conformity has an asymptotic power of 1. For $w \rightarrow \infty$ this implies that there is zero probability of rejecting a true null hypothesis. In practice, however, a large value of $w$ means that we miss out on potentially important dynamics of the system under investigation. At the other extreme, if $w$ is too small, we lack statistical power and thus have a higher false negative rate. As always, $w$ should be chosen considering this trade-off and the application. For our upcoming Twitter case studies (Sections \ref{sec:case-study:-twitter} and \ref{sec:case-study:-outag}), we will see that a common window size ($w=2000$) was sufficient.

The value for $\alpha$ can likewise be user-specified, however we will see in real-world case studies (Figure \ref{fig:case-study-pokemon}) and synthetic experiments (Figure \ref{fig:synthetic-experiments}) that similar results are obtained for the commonly-chosen threshold levels ($\alpha=0.01,0.05$). We use $\alpha=0.05$ in all our work.

\subsection{Performance}
\label{sec:performance}

Assuming a fixed significance threshold $\alpha$ and window size $w$, the K-S critical value can be computed in advance using existing tables for the K-S statistic distribution (for $\alpha=0.05$ it is $\frac{1.358}{\sqrt{w}}$, for example). Our optimized implementation of \algoname{} hence works directly with the K-S statistic, avoiding the need to compute $p$-values.

To calculate the K-S statistic for the current window we would normally need to sort its contents. Using a comparison-based sorting procedure like Heapsort implies a $\mathcal{O}(w \cdot \log w)$ run-time in the worst case for this step. In practice, this sorting only needs to be performed once for the first window. Afterwards, we use standard indexing structures to update our sorted window in $\mathcal{O}(1)$ operations. For each new stream value the calculation of the K-S statistic is hence reduced to $\mathcal{O}(w)$, which is the time to determine the maximum difference between the window's ECDF and the reference uniform distribution.


\section{Case Study: Hashtag Hijacking}
\label{sec:case-study:-twitter}

In the context of social media, Hashtag Hijacking occurs when a hashtag is (ab)used for purposes other than intended. We now demonstrate \algoname{}'s ability to detect this phenomenon in real-time.

<<case-study-hashtag-hijacking-twitter,fig.width=10,fig.height=20,fig.pos='t!',fig.cap='Raw follower counts (top) for users tweeting the hashtag FathersDay. The remaining plots show events, anomalies and changepoints detected by BenFound and other approaches.',echo=F,warning=F>>=
layout(matrix(1:5,5,1)); hashTag <- "FathersDay"; colNumber <- 1; windowStart <- 7;windowEnd <- 84;

## First get the raw signal
fathersDayMetrics <- read.csv("data/stream-2016-05-19-hashtag-FathersDay-metrics.csv",header=F)

## Now extract the raw values in the time range we're interested in, as well as their timestamps
windowStarts <- calculateWindowStarts(as.numeric(fathersDayMetrics[,1]),2000,100);
fathersDayTimeSeriesVector <- as.numeric(fathersDayMetrics[windowStarts[windowStart]:(windowStarts[windowEnd]),colNumber]);
fathersDayTimeSeriesVectorTimestamps <- as.numeric(fathersDayMetrics[windowStarts[windowStart]:(windowStarts[windowEnd]),6]);

streamDates <- seq(as.Date("2016/5/19"), as.Date("2016/7/5"), "days");

## First plot the raw signal (log)
plot(fathersDayTimeSeriesVectorTimestamps, fathersDayTimeSeriesVector, type="l", log="y", ylab="Tweeter Follower Count", xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="Raw Signal (log)",cex.main=3); axis(1,at=as.numeric(as.POSIXct(streamDates,tz="GMT")),labels=format(streamDates,"%b %d"),las=2,cex.axis=2); 

## Now plot the Benfordness signal
ksSignal <- as.numeric(read.csv(sprintf("data/stream-2016-05-19-hashtag-%s-c%d-w2000-i100-ks.csv",hashTag,colNumber),header=FALSE)); timestamps <- as.numeric(read.csv(sprintf("data/stream-2016-05-19-hashtag-%s-c%d-w2000-i100-timestamps.csv",hashTag,colNumber),header=FALSE)); 
plot(timestamps[windowStart:windowEnd],ksSignal[windowStart:windowEnd],type="l",ylab=sprintf("Benfordness (p-value)"),xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="BenFound (w=2000)",cex.main=3); abline(0.05,0,col=8,lty=3,lwd=4); axis(1,at=as.numeric(as.POSIXct(streamDates,tz="GMT")),labels=format(streamDates,"%b %d"),las=2,cex.axis=2); rect(c(1465469979,1465762879,1466085879),c(-1,-1,-1),c(1465723779,1465944879,1466105879),c(1,1,1),col=alpha(2,0.4),lwd=0); 
## abline(v=1465485779,col=8,lty=3,lwd=4);

## Now put in our comparison techniques
## First Twitter Anomaly Detection
fathersDayTwitterAnomalyDetectionFrame <- data.frame(ts=as.POSIXct(fathersDayTimeSeriesVectorTimestamps, origin="1970-01-01",tz="UTC"),val=fathersDayTimeSeriesVector)
fathersDayTwitterAnomalyDetectionResult = AnomalyDetectionVec(fathersDayTwitterAnomalyDetectionFrame[,2], max_anoms=0.005, period=1440, direction='both', plot=T,only_last=F)
fathersDayTwitterAnomalyDetectionResultAnomsIndices <- as.numeric(fathersDayTwitterAnomalyDetectionResult$anoms[,1])
plot(fathersDayTimeSeriesVectorTimestamps, fathersDayTimeSeriesVector, type="l", log="y", ylab="Tweeter Follower Count", xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="Twitter Anomaly Detection",cex.main=3); axis(1,at=as.numeric(as.POSIXct(streamDates,tz="GMT")),labels=format(streamDates,"%b %d"),las=2,cex.axis=2); 
points(fathersDayTimeSeriesVectorTimestamps[fathersDayTwitterAnomalyDetectionResultAnomsIndices],fathersDayTimeSeriesVector[fathersDayTwitterAnomalyDetectionResultAnomsIndices],col=2,pch=21,bg=2,cex=3)

## Now Twitter Breakout Detection
fathersDayTwitterBreakoutResult = breakout(fathersDayTimeSeriesVector, method='multi', plot=FALSE)
plot(fathersDayTimeSeriesVectorTimestamps, fathersDayTimeSeriesVector, type="l", log="y", ylab="Tweeter Follower Count", xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="Twitter Breakout Detection",cex.main=3); axis(1,at=as.numeric(as.POSIXct(streamDates,tz="GMT")),labels=format(streamDates,"%b %d"),las=2,cex.axis=2);

## Finally, EGADS
egadsFathersDayOlympic <- read.csv("data/real-experiments-result-egads-olympic-fathersday.csv",header=F)

egadsFathersDayOlympicTimeIndices <- as.numeric(egadsFathersDayOlympic$V1)
## egadsFathersDayOlympicSignalValues <- as.numeric(egadsFathersDayOlympic$V2)
plot(fathersDayTimeSeriesVectorTimestamps, fathersDayTimeSeriesVector, type="l", log="y", ylab="Tweeter Follower Count", xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="EGADS (Olympic Model) Change-Point Locations",cex.main=3); axis(1,at=as.numeric(as.POSIXct(streamDates,tz="GMT")),labels=format(streamDates,"%b %d"),las=2,cex.axis=2); 
points(fathersDayTimeSeriesVectorTimestamps[egadsFathersDayOlympicTimeIndices],fathersDayTimeSeriesVector[egadsFathersDayOlympicTimeIndices],col=2,pch=21,bg=2,cex=3)

@ 

Consider the hashtag \textbf{\#FathersDay}. The intended purpose of this hashtag is to allow Twitter users to celebrate their father. Father's Day occurred in most countries on 19 June 2016. During the time leading up to Father's Day, we listened for and logged all tweets referencing this hashtag. The raw follower counts of the corresponding tweeters are traced in Figure \ref{fig:case-study-hashtag-hijacking-twitter}. The same Figure shows events, anomalies and change-points found by \algoname{} and three state-of-the-art approaches (\cite{laptev2015egads,james2014twitter,vallis2014novel}, discussed further in Section \ref{sec:synth-exper} and \ref{sec:related-work}). These three approaches consider the \textit{absolute values} of the streamed values (tweeter follower counts).

The \algoname{} signal shows that the behavior begins Benford, turning non-Benford before Father's Day, and finally regressing to Benford \textit{on} Father's Day. Looking at the data, we quickly see the behavior that the Benfordness signal describes. In the lead-up to Father's Day, a large number of spammers and advertising accounts hijacked the hashtag. The follower-counts of these accounts have a non-Benford (unnatural) digit distribution. This behavior is reflected in the low $p$-value of the Benfordness signal. It first drops below our threshold on 10 June 2016\footnote{We note that times were not measured in an American timezone (rather UTC), hence the reason for the slight shift in the data.}, the beginning of the weekend before Father's Day and a logical time for those spammers and advertisers unscrupulously trying to generate profit from the event to begin pushing their content. Example tweets from this time are in Table \ref{tab:sampleTweetsFathersDayLeadUp}.

On Father's Day itself, spammers and advertisers presumably recognized that time had run out to generate profit from this hashtag. The tweet behavior shifted to the more organic kind of content expected for this hashtag. In Table \ref{tab:sampleTweetsFathersDayLeadUp} we find examples of the tweets from 19 June 2016 (users celebrating the efforts of fathers).

The other techniques struggle to extract meaningful information from the raw signal. Two of the techniques find an indigestible number of changepoints/anomalies; the third (Breakout Detection) finds none at all.

\begin{table}[h!]
  \footnotesize
  \centering
  \begin{tabular}{|p{8cm}|}
    \hline
    \textbf{\underline{Father's Day lead-up:}} \\
    \textit{``Oh Yea! I just entered to \#Win LED GlowBowl''}; \textit{``Check out Libbey Ceramic Tiki Mug Blue 7 x 3 16 Ounces''}; \textit{``Enter the \#giveaway to \#win''}; \textit{``I just entered to \#win a \$50 giftcard to @cuffdaddy \#giveaway''}; \\
    \textbf{\underline{Father's Day itself:}} \\
    \textit{``I love you Dad, Happy Father's Day''}; \textit{``The best thing a man can do for his children is to love their mother''}; \textit{``To all of the champion fathers... Happy \#FathersDay!''}; \textit{``Hope you had a great \#FathersDay, lads!''}  \\
    \hline
  \end{tabular}
  \caption{Tweets \textit{leading up to} and \textit{on} Father's Day}
  \label{tab:sampleTweetsFathersDayLeadUp}
\end{table}


\section{Case Study: Outage Detection}
\label{sec:case-study:-outag}

Shortly before the time of data collection, the popular \textbf{\#PokemonGO} game had been released. We tracked the tweets to this hashtag over a number of days. The Benfordness signal is shown in Figure \ref{fig:case-study-pokemon}.

The signal shows a sharp drop on July 15 and 16, hinting that the ``natural'' behavior of tweets for this hashtag had changed. An analysis of the data reveals that the the Pokemon GO application suffered severe outages during this time period. The tweets during this time period were largely revolving around these problems (see Table \ref{tab:sampleTweetsPokemon}) and had a non-Benford digit distribution. The official \textbf{@PokemonGoApp} account confirmed the problems on July 16 (Figure \ref{fig:twitter-pokemongoapp-tweets}). As a comparison, the state-of-the-art Twitter Anomaly Detection result is shown in Figure \ref{fig:case-study-pokemon}, and events/topics detected using two state-of-the-art Twitter text-mining approaches are shown in Table \ref{tab:top10NlpPokemon}.

<<case-study-pokemon,fig.width=10,fig.height=10,fig.pos='t',fig.cap='Tracking the Benfordness of follower-counts associated with users tweeting against the \\#PokemonGO hashtag.',echo=F,warning=F>>=

layout(matrix(1:2,2,1));

pokemonKsSignal <- as.numeric(read.csv("data/stream-2016-07-07-snapshot-2016-07-26-hashtag-PokemonGO-c1-w2000-i100-ks.csv",header=FALSE))
pokemonTimestamps <- as.numeric(read.csv("data/stream-2016-07-07-snapshot-2016-07-26-hashtag-PokemonGO-c1-w2000-i100-timestamps.csv",header=FALSE)); 
pokemonStreamDates <- seq(as.Date("2016/7/6"), as.Date("2016/7/27"), "days");

plot(pokemonTimestamps[130:239],pokemonKsSignal[130:239],type="l",ylab="Benfordness (p-value)",xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="BenFound (w=2000)",cex.main=1.7)
abline(0.05,0,col=8,lty=3,lwd=4)
rect(c(1468538879),c(-1),c(1468657879),c(1),col=alpha(2,0.4),lwd=0); 
axis(1,at=as.numeric(as.POSIXct(pokemonStreamDates,tz="GMT")),labels=format(pokemonStreamDates,"%b %d"),las=2,cex.axis=2)

## Now Twitter Anomaly detection
windowStart <- 129; windowEnd <- 238;
## Now let's apply Twitter AnomalyDetection to the original data in that time frame
pokemonGoMetrics <- read.csv("data/stream-2016-07-07-snapshot-2016-07-26-hashtag-PokemonGO-metrics.csv",header=F)

## Now extract the raw values in the time range we're interested in, as well as their timestamps
windowStarts <- calculateWindowStarts(as.numeric(pokemonGoMetrics[,1]),2000,100);
pokemonGoTimeSeriesVector <- as.numeric(pokemonGoMetrics[windowStarts[windowStart]:(windowStarts[windowEnd]),1]);
pokemonGoTimeSeriesVectorTimestamps <- as.numeric(pokemonGoMetrics[windowStarts[windowStart]:(windowStarts[windowEnd]),6]);

streamDates <- seq(as.Date("2016/5/19"), as.Date("2016/8/5"), "days");

pokemonGoTwitterAnomalyDetectionFrame <- data.frame(ts=as.POSIXct(pokemonGoTimeSeriesVectorTimestamps, origin="1970-01-01",tz="UTC"),val=pokemonGoTimeSeriesVector)

## Here we're filtering to the corresponding timeframe above
pokemonGoTwitterAnomalyDetectionResult = AnomalyDetectionVec(pokemonGoTwitterAnomalyDetectionFrame[,2], max_anoms=0.005, period=1440, direction='both', plot=T,only_last=F)
pokemonGoTwitterAnomalyDetectionResultAnomsIndices <- as.numeric(pokemonGoTwitterAnomalyDetectionResult$anoms[,1])
plot(pokemonGoTimeSeriesVectorTimestamps, pokemonGoTimeSeriesVector, type="l", log="y", ylab="Tweeter Follower Count", xlab="",xaxt="n",cex.axis=2,cex.lab=1.5,main="Twitter Anomaly Detection",cex.main=1.7); axis(1,at=as.numeric(as.POSIXct(streamDates,tz="GMT")),labels=format(streamDates,"%b %d"),las=2,cex.axis=2); 
points(pokemonGoTimeSeriesVectorTimestamps[pokemonGoTwitterAnomalyDetectionResultAnomsIndices], pokemonGoTimeSeriesVector[pokemonGoTwitterAnomalyDetectionResultAnomsIndices],col=2,pch=21,bg=2,cex=2)


## To reproduce the Twitter-NLP result, follow these steps:
## 1. clone the twitter_nlp repository: https://github.com/aritter/twitter_nlp.git
## 2: Run export TWITTER_NLP=./ from the directory! Important!
## 3. From within the directory run: python python/ner/extractEntities.py -p -e data/stream-2016-07-07-snapshot-2016-07-26-hashtag-PokemonGO-tweets-ASCII.csv -o output.csv
## 4. Extract the event phrases: head output.txt -n10000 | grep -oP " [^ ]*B-EVENT ([^ ]*I-EVENT)*" | sed 's/\/[A-Z\-]*//g' > data/output-events.csv
## This takes around 40 minutes or so on a desktop PC.  
twitterNlpEvents <- read.csv("data/stream-2016-07-07-snapshot-2016-07-26-hashtag-PokemonGO-tweets-TwitterNLP-events.csv",header=F)
twitterNlpEventsTableOrdered <- sort(table(events),decreasing=T)
twitterNlpTop10TwoWordEventsPokemon <- head(names(eventsTableOrdered[grepl("[^ ] [^ ]",names(twitterNlpEventsTableOrdered))]),n=10)
twitterNlpTop10TwoWordEventsPokemonCsv <- paste(twitterNlpTop10TwoWordEventsPokemon,collapse=",")
## The above equals "has in, strict parents, bringing out, are down, came over, get off, go down, looking for, been down, get out" which we use in the table below with formatting (which we can't really do nicely using R)

## To reproduce the streamingNMF result, follow these steps:
## First, we need to generate a "user-term" input file and a vocabulary file for input into the streamingNMF algorithm. For this we use the R package "tm" (text mining). Run these commands...
## ## Get the data frame
## pokemonGODF <- read.csv("data/stream-2016-07-07-snapshot-2016-07-26-hashtag-PokemonGO.csv")
## ## Get the tweets character vector
## pokemonTweets <- as.character(pokemonGODF[,9])
## ## Get the user ids vector
## pokemonTweetUserIds <- as.numeric(pokemonGODF[,1])
## ## Get just the unique user ids
## pokemonTweetUserIdsUnique <- unique(pokemonTweetUserIds)
## ## Construct documents (concatenated tweets) per unique user. This is then a character vector, each element of which is the concatenation of one or more tweets for one user.
## pokemonUserTweets <- c(); for(userid in pokemonTweetUserIdsUnique){ tweetIndicesForUser <- which(pokemonTweetUserIds==userid); pokemonUserTweets <- c(pokemonUserTweets, paste(as.character(pokemonTweets[tweetIndicesForUser]),sep=" ",collapse=" "));}
## ## Build the corpus and clean it up with the usual tricks
## pokemonGoTweetCorpus <- Corpus(VectorSource(as.character(pokemonUserTweets)))
## pokemonGoTweetCorpus <- tm_map(pokemonGoTweetCorpus, content_transformer(tolower))
## pokemonGoTweetCorpus <- tm_map(pokemonGoTweetCorpus, removePunctuation)
## pokemonGoTweetCorpus <- tm_map(pokemonGoTweetCorpus, removeWords,c(stopwords("english"),"prolife","prochoice"))
## ## Build the TermDocumentMatrix
## pokemonGoTweetTermDocumentMatrix <- TermDocumentMatrix(pokemonGoTweetCorpus)
## ## Use the TermDocumentMatrix to build a text file that streamingNMF expects. It requires each line of the file to be docId wordId frequency, i.e. three integers separated by a space. In our case documents are users, so we do this by iterating over our unique users
## for(userIndex in seq_along(pokemonTweetUserIdsUnique)){ userElementIndices <- which(pokemonGoTweetTermDocumentMatrix[[2]]==userIndex); terms <- pokemonGoTweetTermDocumentMatrix[[1]][userElementIndices]; frequencies <- pokemonGoTweetTermDocumentMatrix[[3]][userElementIndices]; userMatrix <- matrix(c(replicate(length(terms),userIndex),terms,frequencies),nrow=length(terms),ncol=3,byrow=F); write.table(userMatrix,"data/comparison-technique-data-streamingNMF-pokemonGO-user-term-matrix.txt",append=TRUE,quote=FALSE,sep=" ",row.names=F,col.names=F); }
## We now have the input file, and we now run the following from bash
##head -n303308 /home/sam/Documents/phd/projects/bend-it-like-benford/tex/data/comparison-technique-data-streamingNMF-pokemonGO-user-term-matrix.txt | python parse.py /home/sam/Documents/phd/projects/bend-it-like-benford/tex/data/comparison-technique-data-streamingNMF-pokemonGO-user-term-matrix-vocabulary.txt 1 | python run_nmf.py --rank 10 --delta_topic 5000 --learning_rate 'invsqrt'
## Basically that gets our full input file, pipes it into the streamingNMF parse utility that moulds it into the required input format for run_nmf.py. That then searches for the top 10 topics using the learning rate suggested in the streamingNMF demo. As we have 40000+ "documents" (users), we only report results after 5000 iterations. As a result of this call we get the file data/comparison-technique-data-streamingNMF-out-itr35000topics.txt, which contains our 10 topics that we report in this paper.

@ 

\begin{table}[h!]
  \footnotesize
  \centering
  \begin{tabular}{|p{8cm}|}
    \hline
    \textit{SERVER UPDATE: According to @Independent, \#PokemonGO servers have been taken down in a DDOS attack.} \\
    \textit{The \#PokemonGO servers are down due to a DDOS attack. No word on when they will be back. Stay with us for the latest.} \\
    \textit{Niantic trying to fix the \#PokemonGO servers is as successful as Team Rocket trying to kidnap Pikachu.} \\
    \textit{\#PokemonGO is down. What do I do now with my time?} \\
    \hline
  \end{tabular}
  \caption{Sample of Tweets with the hashtag \#PokemonGO on July 15 and 16, 2016}
  \label{tab:sampleTweetsPokemon}
\end{table}

\begin{table}[h!]
  \footnotesize
  \centering
  \begin{tabular}{|p{8cm}|}
    \hline
    \textbf{Top 10 (Twitter NLP \cite{ritter2012open})}: has in, strict parents, bringing out, \textbf{are down}, came over, get off, \textbf{go down}, looking for, \textbf{been down}, get out \\
    \textbf{Top 10 (Twitter Topic Detection (Streaming NMF) \cite{hayashi2015real})}: [argentina, coins, nintendome], [team, argentina, spark], [pokemongonews, argentina, community], [pokecoins, try, need], [argentina, giveaway, must], [try, \textbf{servers}, pokecoins], [syrian, hopes, saved], [argentina, coins, lucky], [argentina, need, try], [argentina, \textbf{unstable}, lucky]\\
    \hline
  \end{tabular}
  \caption{Top 10 events/topics found by two state-of-the-art Twitter text-mining techniques \cite{hayashi2015real,  ritter2012open}.}
  \label{tab:top10NlpPokemon}
\end{table}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\maxwidth]{graphics/TwitterPostsPokemon.png} \caption[Tweets from the verified \@PokemonGoApp account on July 16, 2016.]{Tweets from the verified \@PokemonGoApp account on July 16, 2016.}\label{fig:twitter-pokemongoapp-tweets}
\end{figure}


\section{Experiments on Synthetic Data}
\label{sec:synth-exper}

In this section we use synthetically-generated numerical time-series data to compare our proposed method with state-of-the-art techniques from the field of numerical event- and anomaly-detection. Our generative model is inspired by our observations from online services such as Wikipedia and Twitter. From our introduction we know that normal usage patterns in these services lead to highly-Benford metrics. In contrast, the set of such values coming from ``unnatural'' usage patterns like bots and hashtag-hijacking is often non-Benford. Our goal is to be alerted when such non-Benford changes occur (denoted a \textit{negative} event), as well as when things return to being Benford (denoted a \textit{positive} event). We make all synthetic models, data and experiments that follow available for reuse\footnotemark[\ref{supMatFootnote}].

We make the simplifying assumption that one interaction (e.g. page edit in Wikipedia or tweet in Twitter) occurs per unit of time. The $n$ interactions with the system fall at times $t_1, \ldots, t_n$. To evaluate the detection of both \textit{negative} and \textit{positive} events, we divide time into \textbf{three intervals}. The first interval spans the range $t_1,\ldots,t_{\lfloor\frac{n}{3}\rfloor}$, the second $t_{\lfloor\frac{n}{3}\rfloor + 1},\ldots,t_{\lfloor\frac{2n}{3}\rfloor}$ and the third $t_{\lfloor\frac{2n}{3}\rfloor + 1},\ldots,t_n$. Inside the first and third intervals we generate ``natural'' (Benford) data. For the second interval we generate non-Benford data. In this way we synthesize a negative event at the transition from the first interval to the second, followed by a positive event at the transition from the second interval to the third.

%% With reference to our \textbf{\#FathersDay} example, we recognize that a shift in the \textit{scale} of the followers distribution is no cause for alarm, as long as the digits pattern continues to obey BL. To simulate this, we thus split the first interval into two equally-sized sub-intervals, the only difference being the value range over which the Benford data is generated. For the second interval we generate non-Benford data by sampling from the uniform. The data generation for the third interval is identical to the first. In this way we synthesize a negative event at the transition from the first interval to the second, followed by a positive event at the transition from the second interval to the third.

To generate Benford data for \textbf{intervals one and three}, we begin with a uniformly-distributed sample $\vec{u} \in [0,1]^s$. With reference to Section \ref{sec:test-conf-benf}, this sample $\vec{u}$ represents our mantissae. Next we generate the sample $\vec{m} \in \left\{0,1,\ldots, 10\right\}^s$ of integers (uniformly sampled with replacement). This sample $\vec{m}$ represents our ``magnitude'' information. Our final Benford set of values then satisfies $\log_{10}(\vec{b}) = \log_{10}(\vec{u} + \vec{m})$, thus $\vec{b} = \left(10^{u_1+m_1}, \ldots, 10^{u_s+m_s} \right)$.

To generate \textit{non-Benford} data for \textbf{interval two}, we follow \cite{berger2011benford} and \textit{uniformly} sample in the range $[0,10^{11}]$.

Figure \ref{fig:synthetic-experiments} compares \algoname{} with 10 different approaches/configurations to event and change-point detection. For this data we are not able to compare with techniques specific for social media (e.g. Twitter) because they rely on the text feed and particular features (hashtags, retweets). \algoname{} is the only approach able to identify the correct number of changepoints (two), their locations and the fact that they form a single ``event''. With respect to \textbf{runtime}, \algoname{}'s bandwidth is 8900 streamed values per second when using our R prototype with window size $w=250$. Doubling to $w=500$ reduces the bandwidth to 7850 streamed values per second. As a real-world reference, we note that around 6000 tweets are tweeted on Twitter every second\footnote{internetlivestats.com/twitter-statistics}.

% REPRODUCING THE BANDWIDTH RESULT STATED ABOVE (e.g. 8900 values per second):
% The following code can be used to simulate an online scenario. The first part just generates some synthetic data (length 6000). The second part actually conducts the experiment with increment=1. Note we have to sort once at the beginning before bringing the stream "online"

% totalLength <- 6000; beta1 <- 10; beta2 <- 10; betaNonBenford <- 10;windowSize <- 250; greenRedGreenColors <- rainbow(3,alpha=0.3); greenRedGreenColors <- c(greenRedGreenColors[2],greenRedGreenColors[1], greenRedGreenColors[2]); set.seed(336); vecU <- seq(0,1,length=totalLength/6); vecM11 <- sample(1:beta1,length(vecU),replace=TRUE); vecM12 <- sample(1:beta2,length(vecU),replace=TRUE); vecM21 <- sample(1:beta1,length(vecU),replace=TRUE); vecM22 <- sample(1:beta2,length(vecU),replace=TRUE); vecB11 <- 10^(vecU+vecM11); vecB12 <- 10^(vecU+vecM12); vecB21 <- 10^(vecU+vecM21); vecB22 <- 10^(vecU+vecM22); perfectBenfordSet11 <- sample(vecB11,length(vecB11)); perfectBenfordSet12 <- sample(vecB12,length(vecB12)); perfectBenfordSet21 <- sample(vecB21,length(vecB21)); perfectBenfordSet22 <- sample(vecB22,length(vecB22)); timeSeriesVector <- c(perfectBenfordSet11, perfectBenfordSet12, runif(totalLength/3,0)*10^(sample(2:(betaNonBenford+1),totalLength/3,replace=TRUE)), perfectBenfordSet21,perfectBenfordSet22); 

% windowStarts <- seq(1,length(timeSeriesVector)-windowSize+1,1); currentWindow <- timeSeriesVector[1:windowSize]; currentWindow <- log(currentWindow, 10)-floor(log(currentWindow,10)); currentWindowSorted <- sort(currentWindow); benfordnessSignalIndex <- 1; benfordnessSignal <- numeric(totalLength-windowSize+1); benfordnessSignal[benfordnessSignalIndex] <- calculateKsOneSidedStatisticUniform(currentWindowSorted); uniformEcdfSteps <- (0:(windowSize - 1))/windowSize; inverseWindowSize <- 1/windowSize; timeStart <- proc.time(); for(newValueIndex in (windowSize+1):length(timeSeriesVector)){ benfordnessSignalIndex <- benfordnessSignalIndex+1; newValue <- timeSeriesVector[newValueIndex]; newValue <- log(newValue, 10)-floor(log(newValue,10)); shiftSortedWindowResult <- shiftSortedWindow(currentWindow,currentWindowSorted,newValue); currentWindow <- shiftSortedWindowResult$newWindowUnsorted; currentWindowSorted <- shiftSortedWindowResult$newWindowSorted; differenceToUniform <- punif(currentWindowSorted) - uniformEcdfSteps; ksStatistic <- max(c(differenceToUniform, inverseWindowSize - differenceToUniform));  benfordnessSignal[benfordnessSignalIndex] <- ksStatistic; }; timeEnd <- proc.time(); plot(seq_along(benfordnessSignal),benfordnessSignal,type="l",main="BenFound (statistic,w=250)",xlab="",xaxt="n",ylab="BenFound (p-value)",cex.main=1.5); abline(1.358/sqrt(windowSize),0,col=8,lty=3,lwd=4); print(totalLength/(as.numeric(timeEnd-timeStart)[3])); 

<<synthetic-experiments-setup,echo=F,warning=F,message=F,errors=F,results="hide",include=F>>=

beta1 <- 10; beta2 <- 10; betaNonBenford <- 10;windowSize <- 250; greenRedGreenColors <- rainbow(3,alpha=0.3); greenRedGreenColors <- c(greenRedGreenColors[2],greenRedGreenColors[1], greenRedGreenColors[2]); set.seed(336); vecU <- seq(0,1,length=1000); vecM11 <- sample(1:beta1,length(vecU),replace=TRUE); vecM12 <- sample(1:beta2,length(vecU),replace=TRUE); vecM21 <- sample(1:beta1,length(vecU),replace=TRUE); vecM22 <- sample(1:beta2,length(vecU),replace=TRUE); vecB11 <- 10^(vecU+vecM11); vecB12 <- 10^(vecU+vecM12); vecB21 <- 10^(vecU+vecM21); vecB22 <- 10^(vecU+vecM22); perfectBenfordSet11 <- sample(vecB11,length(vecB11)); perfectBenfordSet12 <- sample(vecB12,length(vecB12)); perfectBenfordSet21 <- sample(vecB21,length(vecB21)); perfectBenfordSet22 <- sample(vecB22,length(vecB22)); timeSeriesVector <- c(perfectBenfordSet11, perfectBenfordSet12, runif(2000,0)*10^(sample(2:(betaNonBenford+1),2000,replace=TRUE)), perfectBenfordSet21,perfectBenfordSet22); windowStarts <- seq(1,length(timeSeriesVector)-windowSize+1,10); benfordnessSignal <- c(); meanSignal <- c(); for(w in windowStarts){ currentWindow <- timeSeriesVector[w:(w+windowSize - 1)]; ksr <- calculateKS(currentWindow); benfordnessSignal <- c(benfordnessSignal,ksr$p.value); meanSignal <- c(meanSignal,mean(currentWindow));};

synthDataN <- length(timeSeriesVector)

## Binary segmentation result
binSegCptMeanResult <- cpt.mean(timeSeriesVector,Q=2,method="BinSeg")
binSegCptVarResult <- cpt.var(timeSeriesVector,Q=2,method="BinSeg")
binSegCptMeanVarResult <- cpt.meanvar(timeSeriesVector,Q=2,method="BinSeg")

## TED result
edr <- eventDetection(timeSeriesVector,w=250)

## Energy Agglomerative multiple change point analysis
## Note we provide an initial clustering (successive sequences of 50 samples form a cluster) because this method is in O(n^2) time complexity and would otherwise take too long.
aggloMembers <- rep(1:600,times=replicate(600,10)); 
aggloResult <- e.agglo(matrix(timeSeriesVector,length(timeSeriesVector),1,byrow=FALSE),member=aggloMembers);

## Change-points estimation by probabilistically-pruned objective
## Here we tell it there are two changepoints to detect
cp3oResult <- e.cp3o(matrix(timeSeriesVector,length(timeSeriesVector),1,byrow=FALSE),K=2)

## Energy divisive. Here we pass the correct number of change points to detect, which is somewhat unfair, but passing k=NULL seems to be computationally intractable.
divisiveResult <- e.divisive(matrix(timeSeriesVector,length(timeSeriesVector),1,byrow=FALSE),k=2)

## pDPA
pDPAData <- new("CGHdata",Y=timeSeriesVector)
pDPAOpt <- new("CGHoptions")
pDPARes <- uniseg(pDPAData,pDPAOpt)

## EGADS (KDD '15)
## To generate the data set for EGADS we just dumped it to a CSV as follows
## egadsDataFrame <- data.frame(timestamp=1:6000,value=timeSeriesVector)
## write.table(egadsDataFrame,"<FILENAME>",sep=",",row.names=F,quote=F)
## Then we took the sample_config file from the EADS code and set 
## AD_MODEL	AdaptiveKernelDensityChangePointDetector
## OUTPUT  PLOT
## The TS Model we varied as noted in the filename
## And left all other settings unchanged
## We then ran EADS on the data with
## java -Dlog4j.configurationFile=src/test/resources/log4j2.xml -cp lib/OpenForecast-0.5.0.jar:target/egads-jar-with-dependencies.jar com.yahoo.egads.Egads src/test/resources/sample_config.ini <FILENAME>
## The result is stored in the data/synthetic-experiments-result-egads-<modelname>.csv
egadsAutoForecastResultFrame <- read.csv("data/synthetic-experiments-result-egads-autoforecast.csv",header=F)
egadsDoubleExponentialSmoothingResultFrame <- read.csv("data/synthetic-experiments-result-egads-doubleexponentialsmoothing.csv",header=F)
egadsMovingAverageResultFrame <- read.csv("data/synthetic-experiments-result-egads-movingaverage.csv",header=F)
egadsMultipleLinearRegressionResultFrame <- read.csv("data/synthetic-experiments-result-egads-multiplelinearregression.csv",header=F)
egadsNaiveForecastingResultFrame <- read.csv("data/synthetic-experiments-result-egads-naiveforecasting.csv",header=F)
egadsOlympicResultFrame <- read.csv("data/synthetic-experiments-result-egads-olympic.csv",header=F)
egadsPolynomialRegressionResultFrame <- read.csv("data/synthetic-experiments-result-egads-polynomialregression.csv",header=F)
egadsRegressionResultFrame <- read.csv("data/synthetic-experiments-result-egads-regression.csv",header=F)
egadsSimpleExponentialSmoothingResultFrame <- read.csv("data/synthetic-experiments-result-egads-simpleexponentialsmoothing.csv",header=F)
egadsTripleExponentialSmoothingResultFrame <- read.csv("data/synthetic-experiments-result-egads-tripleexponentialsmoothing.csv",header=F)
egadsWeightedMovingAverageResultFrame <- read.csv("data/synthetic-experiments-result-egads-weightedmovingaverage.csv",header=F)
egadsSpectralSmootherResultFrame <- read.csv("data/synthetic-experiments-result-egads-spectralsmoother.csv",header=F)
egadsNullResultFrame <- read.csv("data/synthetic-experiments-result-egads-null.csv",header=F)

## Twitter AnomalyDetectionTs
anomalyDetectionTsFrame <- data.frame(ts=as.POSIXct(seq(from=1471942078,by=60,length.out=length(timeSeriesVector)), origin="1970-01-01",tz="UTC"),val=timeSeriesVector)
anomalyDetectionResStart <- proc.time()
anomalyDetectionRes = AnomalyDetectionTs(anomalyDetectionTsFrame, max_anoms=0.0005, direction='both', plot=FALSE)
anomalyDetectionResEnd <- proc.time()
anomalyDetectionResBandwidth <- synthDataN/as.numeric(anomalyDetectionResEnd-anomalyDetectionResStart)[3]
anomalyDetectionResTimeIndices <- (as.numeric(anomalyDetectionRes$anoms$timestamp)-1471942078)/60

## Extreme values R package
extremeValuesIResult <- getOutliers(timeSeriesVector,method="I",distribution="exponential")
extremeValuesIIResult <- getOutliers(timeSeriesVector,method="II",distribution="exponential")

## Twitter Breakout CP
twitterBreakoutResult = breakout(timeSeriesVector, method='multi', plot=FALSE)


@ 

% Note: if isExtendedVersion false, change the figure* to figure below
<<synthetic-experiments,fig.env='figure',fig.width=10,fig.height=15,fig.pos='h!',fig.cap='A synthetically-generated time signal (top left) with Benford (green), non-Benford (red) and finally Benford (green) segments. BenFound (row 2, col 1) is the only technique able to correctly detect the change in state and its regression.',echo=F,warning=F,message=F,errors=F>>=

if(isExtendedVersion){
    layout(matrix(1:24,6,4),heights=replicate(24,1.5)); 
} else{
layout(matrix(1:12,6,2),heights=replicate(24,1.5));     
}

parMar <- par(mar=c(1,0,2,2))

## Raw signal values (log plot)
plot(seq_along(timeSeriesVector),timeSeriesVector,log="y",type="l",main="Raw Signal (log)",xlab="",xaxt="n",ylab="Signal (log)",cex.main=1.5); 
rect(c(0,2000,4000),c(0.001,0.001,0.001),c(2000,4000,6000),c(10^12,10^12,10^12),col=greenRedGreenColors,lwd=0); 

## Benfordness signal p-values
plot(seq_along(benfordnessSignal),benfordnessSignal,type="l",main="BenFound (p-value,w=250)",xlab="",xaxt="n",ylab="BenFound (p-value)",cex.main=1.5); 
abline(0.05,0,col=8,lty=3,lwd=4); 
rect(c(0,182,390),c(-2,-2,-2),c(182,390,600),c(2,2,2),col=greenRedGreenColors,lwd=0); 

## ## Raw mean signal
## plot(seq_along(meanSignal),meanSignal,type="l",main="Mean",xlab="",xaxt="n",ylab="Mean",cex.main=1.5); 
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(10^10,10^10,10^10),col=greenRedGreenColors,lwd=0); 

## ## Benfordness changepoints
## benfordnessSignalBelowThreshold <- which(benfordnessSignal < 0.05)
## benfordCptSignal <- replicate(length(benfordnessSignal),0)
## benfordCptSignal[benfordnessSignalBelowThreshold] <- 1
## plot(seq_along(benfordCptSignal),benfordCptSignal,type="l",xlab="",xaxt="n",main="BenFound",yaxt="n",ylab="",lwd=4,cex.main=1.5)
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

## Binary segmentation - mean
if(isExtendedVersion){
    binSegCptMeanSignal <- replicate(length(timeSeriesVector),0); 
binSegCptMeanSignal[binSegCptMeanResult@cpts] <- 1; 
plot(seq_along(timeSeriesVector),binSegCptMeanSignal,type="l",xlab="",xaxt="n",main="BinSeg-Mean",yaxt="n",ylab="",lwd=1,cex.main=1.5);
} else{
    binSegCptMeanSignal <- replicate(length(timeSeriesVector),0); 
binSegCptMeanSignal[binSegCptMeanResult@cpts] <- 1; 
plot(seq_along(timeSeriesVector),binSegCptMeanSignal,type="l",xlab="",xaxt="n",main="BinSeg",yaxt="n",ylab="",lwd=1,cex.main=1.5);
}
## rect(c(0,175,400),c(-2,-2,-2),c(175,400,600),c(2,2,2),col=greenRedGreenColors,lwd=0);

if(isExtendedVersion){
## Binary segmentation - var
binSegCptVarSignal <- replicate(length(timeSeriesVector),0); 
binSegCptVarSignal[binSegCptVarResult@cpts] <- 1; 
plot(seq_along(timeSeriesVector),binSegCptVarSignal,type="l",xlab="",xaxt="n",main="BinSeg-Var",yaxt="n",ylab="",lwd=1,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

## Binary segmentation - meanvar
binSegCptMeanVarSignal <- replicate(length(timeSeriesVector),0); 
binSegCptMeanVarSignal[binSegCptMeanVarResult@cpts] <- 1; 
plot(seq_along(timeSeriesVector),binSegCptMeanVarSignal,type="l",xlab="",xaxt="n",main="BinSeg-MeanVar",yaxt="n",ylab="",lwd=1,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);    
}


## TED package (Event Detection)
## We omit this to save space (its results aren't very good anyway)
## eventStarts <- floor(edr$start/10) ## Divide by our increment size to scale to our graph
## eventEnds <- floor(edr$end/10)
## tedSignal <- replicate(length(timeSeriesVector),0); 
## for(i in seq_along(edr$start)){ tedSignal[edr$start[i]:edr$end[i]] <- 1;};
## plot(seq_along(timeSeriesVector),tedSignal,type="l",xlab="",xaxt="n",main="TED (w=250)",yaxt="n",ylab="",lwd=4,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

## Energy agglomerative clustering
aggloChangePoints <- aggloResult$estimates
aggloCptSignal <- replicate(length(timeSeriesVector),0); 
aggloCptSignal[aggloChangePoints] <- 1; 
plot(seq_along(aggloCptSignal),aggloCptSignal,type="l",xlab="",xaxt="n",main="eAgglo",yaxt="n",ylab="",lwd=1,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

## Change points estimation by probabilistically pruned objective
cp3oChangePoints <- cp3oResult$estimates
cp3oCptSignal <- replicate(length(timeSeriesVector),0)
cp3oCptSignal[cp3oChangePoints] <- 1
plot(seq_along(cp3oCptSignal),cp3oCptSignal,type="l",xlab="",xaxt="n",main="cp3o",yaxt="n",ylab="",lwd=1,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

## Energy divisive
divisiveChangePoints <- divisiveResult$estimates
divisiveSignal <- replicate(length(timeSeriesVector),0)
divisiveSignal[divisiveChangePoints] <- 1
plot(seq_along(divisiveSignal),divisiveSignal,type="l",xlab="",xaxt="n",main="eDiv",yaxt="n",ylab="",lwd=1,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

## pDPA
pDPAChangePoints <- head((pDPARes@mu)$Y)$begin
pDPASignal <- replicate(length(timeSeriesVector),0)
pDPASignal[pDPAChangePoints] <- 1
plot(seq_along(pDPASignal),pDPASignal,type="l",xlab="",xaxt="n",main="pDPA",yaxt="n",ylab="",lwd=1,cex.main=1.5);
## rect(c(0,1750,4000),c(-2,-2,-2),c(1750,4000,6000),c(2,2,2),col=greenRedGreenColors,lwd=0);

if(isExtendedVersion){
## EGADS Auto Forecast
egadsAutoForecastTimeValues <- as.numeric(egadsAutoForecastResultFrame$V1)
egadsAutoForecastSignalValues <- as.numeric(egadsAutoForecastResultFrame$V2)
plot(egadsAutoForecastTimeValues, egadsAutoForecastSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Auto Forecast)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS Double Exponential Smoothing
egadsDoubleExponentialSmoothingTimeValues <- as.numeric(egadsDoubleExponentialSmoothingResultFrame$V1)
egadsDoubleExponentialSmoothingSignalValues <- as.numeric(egadsDoubleExponentialSmoothingResultFrame$V2)
plot(egadsDoubleExponentialSmoothingTimeValues, egadsDoubleExponentialSmoothingSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Double Exp. Smth.)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS Moving Average
egadsMovingAverageTimeValues <- as.numeric(egadsMovingAverageResultFrame$V1)
egadsMovingAverageSignalValues <- as.numeric(egadsMovingAverageResultFrame$V2)
plot(egadsMovingAverageTimeValues, egadsMovingAverageSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Moving Average)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS Multiple Linear Regression
egadsMultipleLinearRegressionTimeValues <- as.numeric(egadsMultipleLinearRegressionResultFrame$V1)
egadsMultipleLinearRegressionSignalValues <- as.numeric(egadsMultipleLinearRegressionResultFrame$V2)
plot(egadsMultipleLinearRegressionTimeValues, egadsMultipleLinearRegressionSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Mult. Lin. Regr.)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS NaiveForecasting
egadsNaiveForecastingTimeValues <- as.numeric(egadsNaiveForecastingResultFrame$V1)
egadsNaiveForecastingSignalValues <- as.numeric(egadsNaiveForecastingResultFrame$V2)
plot(egadsNaiveForecastingTimeValues, egadsNaiveForecastingSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Naive Forecasting)",yaxt="n",ylab="",lwd=1,cex.main=1.5)    
}

## EGADS Olympic
egadsOlympicTimeValues <- as.numeric(egadsOlympicResultFrame$V1)
egadsOlympicSignalValues <- as.numeric(egadsOlympicResultFrame$V2)
plot(egadsOlympicTimeValues, egadsOlympicSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Olympic)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

if(isExtendedVersion){
## EGADS PolynomialRegression
egadsPolynomialRegressionTimeValues <- as.numeric(egadsPolynomialRegressionResultFrame$V1)
egadsPolynomialRegressionSignalValues <- as.numeric(egadsPolynomialRegressionResultFrame$V2)
plot(egadsPolynomialRegressionTimeValues, egadsPolynomialRegressionSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Poly. Regr.)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## ## EGADS Regression
## egadsRegressionTimeValues <- as.numeric(egadsRegressionResultFrame$V1)
## egadsRegressionSignalValues <- as.numeric(egadsRegressionResultFrame$V2)
## plot(egadsRegressionTimeValues, egadsRegressionSignalValues, xlim=c(1,6000),type="l",xlab="",xaxt="n",main="EGADS (Regression)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS Simple Exponential Smoothing
egadsSimpleExponentialSmoothingTimeValues <- as.numeric(egadsSimpleExponentialSmoothingResultFrame$V1)
egadsSimpleExponentialSmoothingSignalValues <- as.numeric(egadsSimpleExponentialSmoothingResultFrame$V2)
plot(egadsSimpleExponentialSmoothingTimeValues, egadsSimpleExponentialSmoothingSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Simp. Exp. Smth.)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS Triple Exponential Smoothing
egadsTripleExponentialSmoothingTimeValues <- as.numeric(egadsTripleExponentialSmoothingResultFrame$V1)
egadsTripleExponentialSmoothingSignalValues <- as.numeric(egadsTripleExponentialSmoothingResultFrame$V2)
plot(egadsTripleExponentialSmoothingTimeValues, egadsTripleExponentialSmoothingSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Triple Exp. Smth.)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS WeightedMovingAverage
egadsWeightedMovingAverageTimeValues <- as.numeric(egadsWeightedMovingAverageResultFrame$V1)
egadsWeightedMovingAverageSignalValues <- as.numeric(egadsWeightedMovingAverageResultFrame$V2)
plot(egadsWeightedMovingAverageTimeValues, egadsWeightedMovingAverageSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Weight. Mov. Avg.)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS SpectralSmoother
egadsSpectralSmootherTimeValues <- as.numeric(egadsSpectralSmootherResultFrame$V1)
egadsSpectralSmootherSignalValues <- as.numeric(egadsSpectralSmootherResultFrame$V2)
plot(egadsSpectralSmootherTimeValues, egadsSpectralSmootherSignalValues, xlim=c(1,6000),type="h",xlab="",xaxt="n",main="EGADS (Spectral Smoother)",yaxt="n",ylab="",lwd=1,cex.main=1.5)

## EGADS Null
## egadsNullTimeValues <- as.numeric(egadsNullResultFrame$V1)
## egadsNullSignalValues <- as.numeric(egadsNullResultFrame$V2)
## plot(egadsNullTimeValues, egadsNullSignalValues, xlim=c(1,6000),type="l",xlab="",xaxt="n",main="EGADS (Null)",yaxt="n",ylab="",lwd=1,cex.main=1.5)    
}

## Twitter Anomaly Detection: https://github.com/twitter/AnomalyDetection
## Note that here we simulate each measurement on a minute-by-minute scale to match their simple example
## We also set the anomalies percentage to 0.0005, which is roughly 2/6000, i.e. the number of anomalies we expect
twitterAnomalyDetectionSignal <- replicate(length(timeSeriesVector),0); 
twitterAnomalyDetectionSignal[anomalyDetectionResTimeIndices] <- 1; 
plot(seq_along(timeSeriesVector),twitterAnomalyDetectionSignal,type="h",xlab="",xaxt="n",main="Twitter Anomaly Detection",yaxt="n",ylab="",lwd=1,cex.main=1.5);

## Twitter Breakout
twitterBreakoutSignal <- replicate(length(timeSeriesVector),0); 
twitterBreakoutSignal[twitterBreakoutResult$loc] <- 1
plot(seq_along(timeSeriesVector),twitterBreakoutSignal,type="l",xlab="",xaxt="n",main="Twitter Breakout Detection",yaxt="n",ylab="",lwd=1,cex.main=1.5);

## Extreme Values R package
extremeValuesISignal <- replicate(length(timeSeriesVector),0); 
extremeValuesISignal[c(extremeValuesIResult$iLeft,extremeValuesIResult$iRight)] <- 1
plot(seq_along(timeSeriesVector),extremeValuesISignal,type="l",xlab="",xaxt="n",main="Extreme Values (R) Method I",yaxt="n",ylab="",lwd=1,cex.main=1.5);

## Extreme Values R package...method II
extremeValuesIISignal <- replicate(length(timeSeriesVector),0); 
extremeValuesIISignal[c(extremeValuesIIResult$iLeft,extremeValuesIIResult$iRight)] <- 1
plot(seq_along(timeSeriesVector),extremeValuesIISignal,type="l",xlab="",xaxt="n",main="Extreme Values (R) Method II",yaxt="n",ylab="",lwd=1,cex.main=1.5);

@ 



%% Our real-world \textbf{#FathersDay} example showed, however, that the non-Benfordness in the \textit{lead-up} to Father's Day was explained by spammers hijacking the hashtag for advertising reasons. After Father's Day, the target for primarily ``organic'' tweets \textit{after} Father's Day.  

\section{Related Work and Discussion}
\label{sec:related-work}

BL was investigated in the context of online services for the first time in 2015 \cite{golbeck2015}. The primary contribution was to show that snapshots of randomly-sampled user metrics in \textit{social networks} (Facebook, Twitter, Pinterest) are Benford. The measurements in our introduction support this conclusion. In addition, we have presented results showing that metrics from Wikipedia, YouTube and GitHub are likewise Benford, and focused on the \textit{real-time} detection of BL \textit{deviations} in such systems.

A small set of dedicated BL reference volumes now exist. For theoretical and practical perspectives we respectively refer to \cite{berger2015Intro} and \cite{nigrini2012}. The latter focuses heavily on BL in forensic accounting, auditing and fraud detection. It treats BL-conformity tests and also compares BL to the well-known Zipf's Law, however it does not treat event-detection. In Section \ref{sec:test-conf-benf} we reevaluate the conformity tests and propose an alternative based on the formal Kolmogorov-Smirnov test.

The topics of event-, anomaly- and changepoint-detection have seen numerous contributions. Focusing firstly on Twitter-specific techniques, we find in \cite{ritter2012open} and \cite{hayashi2015real} two state-of-the-art approaches for the extraction of event phrases and topics from Twitter. Unlike \algoname{}, both rely on processing the tweet text stream. Additionally, whilst \algoname{} only raises a ``red flag'' when the digit distribution violates BL, these techniques \textit{continuously} yield a set of e.g. top-10 topics.

As \algoname{} is not restricted to text and graph structures, it is by no means restricted to social networks. \algoname{} can be applied to any numerical data stream obeying BL. In searching for comparison techniques we therefore extend our scope to the state-of-the-art in \textit{numerical} anomaly- and changepoint-detection from data-mining and statistics.

The most recent high-level contribution from the data-mining community is named the Extensible Generic Anomaly Detection System (EGADS in Figures \ref{fig:case-study-hashtag-hijacking-twitter} and \ref{fig:synthetic-experiments}) and stems from work at Yahoo \cite{laptev2015egads}. EGADS is a comprehensive framework that supports anomaly-detection based on a variety of configurable models, features and metrics. Two further approaches come from Twitter \cite{james2014twitter,vallis2014novel}. Twitter Anomaly Detection \cite{james2014twitter}, for example, employs time-series decomposition using statistical metrics to detect both global and local anomalies in the presence of seasonality and an underlying trend.

From the recent statistical literature we find in \cite{matteson2013} the non-parametric approach \eDiv{} that automatically detects both the number and location of change points. Unlike previous methods which typically focus on detecting a change in mean, variance or kurtosis, \eDiv{} is able to detect any distributional change. From the same work we find \eAgglo{}, the bottom-up variant of \eDiv{}, and another probabilistic pruning method \cpppo{}. These three methods are compared to \algoname{} in Figure \ref{fig:synthetic-experiments}. Each is designed for an offline setting and has a quadratic time complexity in the sample size $n$. %% Segments are found in a divisive hierarchical manner through the use of a binary bisection method. which merges segments such as to maximize a goodness-of-fit statistic.

The pruned dynamic programming algorithm \cite{rigaill2010} (\pDPA{} in Figure \ref{fig:synthetic-experiments}) is an offline approach which aims to infer both the number and positions of the change-points from the data. It uses a functional cost representation of segments in order to prune the search space of the segment neighborhood. It requires a user-specified parameter $k_{max}$ which represents the maximum number of change points to find. Its computational complexity is quadratic in the number of data points $n$.

Finally, we compare to Binary Segmentation \cite{scott1974Grouping,edwards1965cluster} (\BinSeg{} in Figure \ref{fig:synthetic-experiments}). This early approach detects changes in the mean and is commonly used as a baseline. 

                  %                   The approach aims to detect changes in a distribution's mean (\BinSegMean{}), variance (\BinSegVar{}) or both (\BinSegMeanVar{}). %%It exploits the hierarchical clustering technique of Edwards and Cavalli-Sforza \cite{edwards1965cluster} to split the time series into groups. 

                  %%                   The general problem concerns the inference of a \textit{change in the underlying distribution} that is responsible for generating a stream of time-ordered observations. Approaches to solving the problem can be divided into online and offline. In the online case, detection is done adaptively in real-time as new data arrives continuously. In the offline case, an entire history of time-ordered records is analyzed retrospectively. Approaches can further be characterized as parametric or non-parametric, and focus either on single or multiple changepoint detection. 


                  %%                   The Turbulence Time Series Event Detection and Classification (\TED{}) technique \cite{kang2014detecting} uses a two-step approach which extracts events from background fluctuations and groups dynamically-similar events into clusters. In contrast to the aforementioned methods which yield ``change-points'', \TED{} yields more precise ``events'' which encapsulate a start and end time. Like \algoname{}, \TED{} requires a user-specified sliding-window-size parameter.

Importantly, all of these approaches consider the distribution of the \textit{complete} numerical values in question. \algoname{}, in contrast, focuses on a particular kind of change event: a change in its leading digit distribution. This is achieved by considering BL and purposefully \textit{ignoring the magnitude information} of the temporal measurements. Fundamentally then, \algoname{} distances itself from the discussed state-of-the-art in that it does not look for changes to the underlying distribution of \textit{complete} values, but rather that of only the \textit{leading digits}. For this reason, \algoname{} has been able to correctly identify events in synthetic and real data (Sections \ref{sec:case-study:-twitter}, \ref{sec:case-study:-outag} and \ref{sec:synth-exper}) that are not found by any of the comparison techniques.

                  %                   In this way \algoname{} will not raise false alarms if measurements change magnitude but retain their ``natural'' digit distribution.

                  %                   If we focus on event detection in Twitter alone, a recent survey reviews a number of contributions \cite{atefeh2013Survey}. Five techniques are completely unsupervised, do not specify the type of event, and address the task of \textit{new} event detection. All five of these techniques, however, are dependent on the text stream and Twitter-specific features like Hashtags and retweet-counts.

                  %                   Zipf's law is a further empirical phenomenon that states that the frequency of utterances of any word in a natural language corpus is inversely proportional to its rank in the frequency table. Generally, data that conforms to BL does not conform to Zipf's Law and vice versa (\cite{nigrini2012}, Chapter 2). This is seen in a log-log plot of values versus ranks of our Twitter followers data\footnotemark[\ref{supMatFootnote}].

                  %                   <<zipf-plot,fig.width=10,fig.height=10,fig.pos='h',fig.cap='Log-log plot of value versus rank for our Twitter followers data.',echo=F,warning=F>>=

                  %                   ## Here we comment this out for space reasons. Uncomment this here and rebuild to see the plot.
                  %                   ## followersCountVectorOrdered <- followersCountVector[order(followersCountVector,decreasing=T)]
                  %                   ## plot(log(seq_along(followersCountVectorOrdered),base=10),log(followersCountVectorOrdered,base=10))
                  %                   @ 

Finally, our technique has \textbf{limitations}. Firstly, it is clearly only applicable to numeric data that obeys BL, meaning that it is inappropriate for many applications. Secondly, it requires a manual inspection step to analyze the cause for found events. Finally, it is not immune to false negatives. That is, if parties that artificially manipulate a system are aware of BL, they may tune their interactions such that the system metrics are not significantly affected from a BL perspective.

\section{Conclusion}

Various metrics from online services such as Twitter, Wikipedia, YouTube and GitHub naturally obey BL. When these metrics violate BL in real-time, it is often a sign of significant ``unnatural'' behavior. In this work we have proposed \algoname{}, a real-time event-detection approach for ``red-flagging'' such violations. In the case of online services, the interactions may be anti-social or malicious in nature, like non-permitted bot activity or hashtag-hijacking. We have compared \algoname{} to state-of-the-art event-detection techniques in controlled settings with synthetic data. Finally, we have deployed our technique to real-world settings and demonstrated practical knowledge discovery in various domains. \algoname{} is non-parametric, efficient and can be deployed in real-time on numerical data streams that obey BL.


\begin{thebibliography}{99}

\bibitem{alexander2009remarks}
  J.~Alexander, ``Remarks on the use of Benford’s Law'', 2009 (available at SSRN 1505147).

\bibitem{antoine2015portraying}
  {\'E}.~Antoine, A.~Jatowt, S.~Wakamiya, Y.~Kawai, and T.~Akiyama, ``Portraying collective spatial attention in twitter'', KDD 2015.

  
  
                  %                   \bibitem{atefeh2013Survey}
                  %                   F.~Atefeh, and W.~Khreich, ``A Survey of Techniques for Event Detection in Twitter'', in \emph{Computational Intelligence}, vol. 31, no. 1, 2015, pp. 132--164.
  
\bibitem{benford1938law}
  F.~Benford, ``The law of anomalous numbers'', in \emph{Proceedings of the American Philosophical Society}, vol. 78, 1938, pp. 551--572.

\bibitem{berger2011benford}
  A.~Berger, and T.P.~Hill. ``Benford’s Law strikes back: No simple explanation in sight'', in \emph{The Mathematical Intelligencer}, vol. 33, 2011, pp. 85--91.
  
\bibitem{berger2015Intro}
  A.~Nigrini, and T.P.~Hill, \emph{An Introduction to Benford's Law}. Princeton University Press, 2015.

\bibitem{edwards1965cluster}
  A.~Edwards and L.~Cavalli-Sforza, ``A Method for Cluster Analysis'', in \emph{Biometrics}, vol. 21, no. 2, 1965.
  
  %% \bibitem{kang2014detecting}
  %%   Y.~Kang, D.~Belu{\v{s}}i{\'c} and K.~Smith-Miles, ``Detecting and classifying events in noisy time series'', in \emph{Journal of the Atmospheric Sciences}, vol. 71, no. 3, 2014, pp. 1090--1104.

\bibitem{hayashi2015real}
  K.~Hayashi, T.~Maehara, M.~Toyoda and K.~Kawarabayashi, ``Real-time topic detection on twitter with topic hijack filtering'', KDD 2015.

\bibitem{james2014twitter}
  NA.~James, AK.~Kejariwal and DS.~Matteson, ``Leveraging Cloud Data to Mitigate User Experience
  from ‘Breaking Bad’'', arXiv preprint arXiv:1411.7955, 2014.
  
                  %                   \bibitem{killick2012optimal}
                  %                   R.~Killick, P.~Fearnhead and IA.~Eckley, ``Optimal detection of changepoints with a linear computational cost'',in \emph{Journal of the American Statistical Association}, vol. 107, no. 500, 2012, pp. 1590--1598.  
  
\bibitem{kolmogorov1933}
  A.~Kolmogorov. ``Sulla determinazione empirica di una legge di distribuzione''. Italian Actuarial Journal, 1933.

\bibitem{laptev2015egads}
  N.~Laptev, S.~Amizadeh, and I.~Flint, ``Generic and scalable framework for automated time-series anomaly detection'', KDD 2015.
  
\bibitem{matteson2013}
  D.~Matteson and N.~James. ``A Nonparametric Approach for Multiple Change Point Analysis of Multivariate Data'', in \emph{Journal of the American Statistical Association}, vol. 109, no. 505, 2014, pp.334-345.
  
\bibitem{nigrini2012}
  M.J.~Nigrini, \emph{Benford's Law: Applic. for Forensic Acc., Auditing and Fraud Det.}. John Wiley \& Sons, 2012.
  
                                                                                                        %                                                                                                         \bibitem{lemire2015}
                                                                                                        %                                                                                                         D.~Lemire, and L.~Boytsov. ``Decoding billions of integers per second through vectorization'', in \emph{Software: Practice and Experience}, vol. 45, 2015,pp. 1--29.
  
\bibitem{golbeck2015}
  J.~Golbeck. ``Benford's Law Applies to Online Social Networks'', in \emph{PloS one}, vol. 10, 2015.

\bibitem{rigaill2010}
  G.~Rigaill, ``Pruned Dynamic Programming for Optimal Multiple Change-Point Detection'', arXiv:1004.0887.
  
\bibitem{ritter2012open}
  A.~Ritter, Mausam, O.~Etzioni, S.~Clark, ``Open Domain Event Extraction from Twitter'', KDD 2012.

\bibitem{scott1974Grouping}
  A.~Scott and M.~Knott, ``A Cluster Analysis Method for Grouping Means in the Analysis of Variance'', in \emph{Biometrics}, vol. 30, no. 3, 1974, pp. 507--512.
  
\bibitem{twitterTos}
  Twitter Inc. ``Twitter Terms of Service''. Online Resource. https://www.twitter.com/tos.

\bibitem{vallis2014novel}
  O.~Vallis, J.~Hochenbaum and A.~Kejariwal, ``A novel technique for long-term anomaly detection in the cloud'', in 6th USENIX (HotCloud 2014).

  
\end{thebibliography}


\end{document}

                                                                                                        %                                                                                                         End of ltexpprt.tex
